{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1ba6376",
   "metadata": {},
   "source": [
    "# Árbol de decisión para la regresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef18d410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "penguins = pd.read_csv(\"../../data/penguins/penguins_regression.csv\")\n",
    "\n",
    "\n",
    "feature_name = \"Flipper Length (mm)\"\n",
    "target_name = \"Body Mass (g)\"\n",
    "data_train, target_train = penguins[[feature_name]], penguins[target_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd248da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data_test = pd.DataFrame(np.arange(data_train[feature_name].min(),\n",
    "                                   data_train[feature_name].max()),\n",
    "                                   columns=[feature_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c7fc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.scatterplot(data=penguins, x=feature_name, y=target_name,\n",
    "                color=\"black\", alpha=0.5)\n",
    "_ = plt.title(\"Ilustración del dataset de regresión usado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df403922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero ilustraremos la diferencia entre un modelo lineal y un árbol de decisión.\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(data_train, target_train)\n",
    "target_predicted = linear_model.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b103aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=penguins, x=feature_name, y=target_name,\n",
    "                color=\"black\", alpha=0.5)\n",
    "plt.plot(data_test[feature_name], target_predicted, label=\"Linear regression\")\n",
    "plt.legend()\n",
    "_ = plt.title(\"Función de predicción utilizando LinearRegression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f37485c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(data=penguins, x=feature_name, y=target_name,\n",
    "                     color=\"black\", alpha=0.5)\n",
    "plt.plot(data_test[feature_name], target_predicted, label=\"Linear regression\",\n",
    "         linestyle=\"--\")\n",
    "plt.scatter(data_test[::3], target_predicted[::3], label=\"Predictions\",\n",
    "            color=\"tab:orange\")\n",
    "plt.legend()\n",
    "_ = plt.title(\"Función de predicción utilizando LinearRegression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c46f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los árboles de decisión son modelos no paramétricos.\n",
    "# Repetir el experimento anterior resaltará las diferencias.\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree = DecisionTreeRegressor(max_depth=1)\n",
    "tree.fit(data_train, target_train)\n",
    "target_predicted = tree.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d22302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=penguins, x=feature_name, y=target_name,\n",
    "                color=\"black\", alpha=0.5)\n",
    "plt.plot(data_test[feature_name], target_predicted, label=\"Decision tree\")\n",
    "plt.legend()\n",
    "_ = plt.title(\"Función de predicción utilizando DecisionTreeRegressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda811e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# veamos la estructura del árbol para ver cuál fue el umbral que se encontró durante el entrenamiento.\n",
    "\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "_, ax = plt.subplots(figsize=(8, 6))\n",
    "_ = plot_tree(tree, feature_names=[feature_name], ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc020e4",
   "metadata": {},
   "source": [
    "> El umbral para nuestra característica (longitud de la aleta) es de 206.5 mm.\n",
    "    - Los valores predichos en cada lado de la división son dos constantes: 3698.71 gy 5032.36 g.\n",
    "    - Estos valores corresponden a los valores medios de las muestras de entrenamiento en cada partición.\n",
    "\n",
    "En la clasificación, vimos que aumentar la profundidad del árbol nos permitió obtener límites de decisión más complejos.\n",
    "\n",
    "Verifiquemos el efecto de aumentar la profundidad en una configuración de regresión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca2e316",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeRegressor(max_depth=3)\n",
    "tree.fit(data_train, target_train)\n",
    "target_predicted = tree.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb61670",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=penguins, x=feature_name, y=target_name,\n",
    "                color=\"black\", alpha=0.5)\n",
    "plt.plot(data_test[feature_name], target_predicted, label=\"Decision tree\")\n",
    "plt.legend()\n",
    "_ = plt.title(\"Función de predicción utilizando DecisionTreeRegressor\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
