{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43af74df",
   "metadata": {},
   "source": [
    "# Hiperparámetros en árboles de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa25d8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos los conjuntos de datos de clasificación y regresión.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data_clf_columns = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]\n",
    "target_clf_column = \"Species\"\n",
    "data_clf = pd.read_csv(\"../../data/penguins/penguins_classification.csv\")\n",
    "\n",
    "data_reg_columns = [\"Flipper Length (mm)\"]\n",
    "target_reg_column = \"Body Mass (g)\"\n",
    "data_reg = pd.read_csv(\"../../data/penguins/penguins_regression.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f0a067",
   "metadata": {},
   "source": [
    "# Crear funciones auxiliares\n",
    "Crearemos algunas funciones auxiliares para graficar los datos, así como el límite de decisión (para la clasificación) y la línea de regresión (para la regresión)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2c2a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "\n",
    "def fit_and_plot_classification(model, data, feature_names, target_names):\n",
    "    model.fit(data[feature_names], data[target_names])\n",
    "    if data[target_names].nunique() == 2:\n",
    "        palette = [\"tab:red\", \"tab:blue\"]\n",
    "    else:\n",
    "        palette = [\"tab:red\", \"tab:blue\", \"black\"]\n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        model, data[feature_names], response_method=\"predict\",\n",
    "        cmap=\"RdBu\", alpha=0.5\n",
    "    )\n",
    "    sns.scatterplot(data=data, x=feature_names[0], y=feature_names[1],\n",
    "                    hue=target_names, palette=palette)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "\n",
    "def fit_and_plot_regression(model, data, feature_names, target_names):\n",
    "    model.fit(data[feature_names], data[target_names])\n",
    "    data_test = pd.DataFrame(\n",
    "        np.arange(data.iloc[:, 0].min(), data.iloc[:, 0].max()),\n",
    "        columns=data[feature_names].columns,\n",
    "    )\n",
    "    target_predicted = model.predict(data_test)\n",
    "\n",
    "    sns.scatterplot(\n",
    "        x=data.iloc[:, 0], y=data[target_names], color=\"black\", alpha=0.5)\n",
    "    plt.plot(data_test.iloc[:, 0], target_predicted, linewidth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f5e0e8",
   "metadata": {},
   "source": [
    "# Efecto del parámetro max_depth\n",
    "- El hiperparámetro `max_depth` controla la complejidad general de un árbol de decisión. \n",
    "- Este hiperparámetro permite obtener una compensación entre un árbol de decisión subajustado y sobreajustado. \n",
    "\n",
    "Construiremos un árbol poco profundo y luego un árbol más profundo, tanto para la clasificación como para la regresión, para comprender el impacto del parámetro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7278d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# podemos establecer el valor del parámetro max_depth en un valor muy bajo.\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "\n",
    "max_depth = 2\n",
    "tree_clf = DecisionTreeClassifier(max_depth=max_depth)\n",
    "tree_reg = DecisionTreeRegressor(max_depth=max_depth)\n",
    "\n",
    "fit_and_plot_classification(\n",
    "    tree_clf, data_clf, data_clf_columns, target_clf_column)\n",
    "_ = plt.title(f\"Árbol de clasificación poco profundo con max-depth de {max_depth}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88858d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_and_plot_regression(\n",
    "    tree_reg, data_reg, data_reg_columns, target_reg_column)\n",
    "_ = plt.title(f\"Árbol de regresión poco profundo con max-depth de {max_depth}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e4dade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aumentamos el valor del parámetro max_depth para verificar la diferencia observando la función de decisión.\n",
    "\n",
    "max_depth = 30\n",
    "tree_clf = DecisionTreeClassifier(max_depth=max_depth)\n",
    "tree_reg = DecisionTreeRegressor(max_depth=max_depth)\n",
    "\n",
    "fit_and_plot_classification(\n",
    "    tree_clf, data_clf, data_clf_columns, target_clf_column)\n",
    "_ = plt.title(f\"Árbol de clasificación profunda con max-depth de {max_depth}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5404d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_and_plot_regression(\n",
    "    tree_reg, data_reg, data_reg_columns, target_reg_column)\n",
    "_ = plt.title(f\"Árbol de regresión profunda con max-depth de {max_depth}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e10838",
   "metadata": {},
   "source": [
    "> Para las configuración de clasificación y regresión, observamos que aumentar la profundidad hará que el modelo de árbol sea más expresivo.\n",
    "- Sin embargo, un árbol que es demasiado profundo sobreajustará los datos de entrenamiento, creando particiones que solo son correctas para \"valores atípicos (muestras ruidosas)\".\n",
    "\n",
    "`max_depth` es uno de los hiperparámetros que se debe optimizar a través de la validación cruzada y la búsqueda de cuadrícula.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33414a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\"max_depth\": np.arange(2, 10, 1)}\n",
    "tree_clf = GridSearchCV(DecisionTreeClassifier(), param_grid=param_grid)\n",
    "tree_reg = GridSearchCV(DecisionTreeRegressor(), param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaadc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_and_plot_classification(\n",
    "    tree_clf, data_clf, data_clf_columns, target_clf_column)\n",
    "_ = plt.title(f\"Profundidad óptima encontrada a través de CV: \"\n",
    "              f\"{tree_clf.best_params_['max_depth']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff594569",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_and_plot_regression(\n",
    "    tree_reg, data_reg, data_reg_columns, target_reg_column)\n",
    "_ = plt.title(f\"Profundidad óptima encontrada a través de CV: \"\n",
    "              f\"{tree_reg.best_params_['max_depth']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1de450",
   "metadata": {},
   "source": [
    "# Otros hiperparámetros en los árboles de decisión\n",
    "- El hiperparámetro `max_depth` controla la complejidad general del árbol.\n",
    "- Este parámetro es adecuado bajo el supuesto de que un árbol se construye simétricamente. \n",
    "- Sin embargo, **no hay garantía de que un árbol sea simétrico**.\n",
    "    - De hecho, se podría alcanzar un rendimiento de generalización óptimo al ahce crecer algunas ramas a más profundidad que otras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cbfd78",
   "metadata": {},
   "source": [
    "Construiremos un conjunto de datos donde ilustraremos esta asimetría.\n",
    "- Generaremos un conjunto de datos compuesto por 2 subconjuntos: \n",
    "    - un subconjunto donde el árbol debe encontrar una separación clara \n",
    "    - y otro subconjunto donde se mezclarán muestras de ambas clases.\n",
    "- Implica que un árbol de decisión necesitará más divisiones para clasificar las muestras adecuadamente en el segundo subconjunto respecto al primer subconjunto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c20691d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "data_clf_columns = [\"Feature #0\", \"Feature #1\"]\n",
    "target_clf_column = \"Class\"\n",
    "\n",
    "# Blobs que serán entrelazadas\n",
    "X_1, y_1 = make_blobs(\n",
    "    n_samples=300, centers=[[0, 0], [-1, -1]], random_state=0)\n",
    "# Blobs que se separarán fácilmente\n",
    "X_2, y_2 = make_blobs(\n",
    "    n_samples=300, centers=[[3, 6], [7, 0]], random_state=0)\n",
    "\n",
    "X = np.concatenate([X_1, X_2], axis=0)\n",
    "y = np.concatenate([y_1, y_2])\n",
    "data_clf = np.concatenate([X, y[:, np.newaxis]], axis=1)\n",
    "data_clf = pd.DataFrame(\n",
    "    data_clf, columns=data_clf_columns + [target_clf_column])\n",
    "data_clf[target_clf_column] = data_clf[target_clf_column].astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc229d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=data_clf, x=data_clf_columns[0], y=data_clf_columns[1],\n",
    "                hue=target_clf_column, palette=[\"tab:red\", \"tab:blue\"])\n",
    "_ = plt.title(\"Dataset sintético\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af862562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrenaremos un árbol de decisión poco profundo con max_depth = 2.\n",
    "# Esperaríamos que esta profundidad sea suficiente para separar los blobs que son fáciles de separar.\n",
    "\n",
    "max_depth = 2\n",
    "tree_clf = DecisionTreeClassifier(max_depth=max_depth)\n",
    "fit_and_plot_classification(\n",
    "    tree_clf, data_clf, data_clf_columns, target_clf_column)\n",
    "_ = plt.title(f\"Árbol de decisión con max-depth de {max_depth}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050db211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# la representación del árbol.\n",
    "\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "_, ax = plt.subplots(figsize=(10, 10))\n",
    "_ = plot_tree(tree_clf, ax=ax, feature_names=data_clf_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfeef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos que la rama correcta logra una clasificación perfecta. \n",
    "# Ahora, aumentamos la profundidad para verificar cómo crecerá el árbol.\n",
    "\n",
    "max_depth = 6\n",
    "tree_clf = DecisionTreeClassifier(max_depth=max_depth)\n",
    "fit_and_plot_classification(\n",
    "    tree_clf, data_clf, data_clf_columns, target_clf_column)\n",
    "_ = plt.title(f\"Árbol de decisión con max-depth de {max_depth}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d6482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(11, 7))\n",
    "_ = plot_tree(tree_clf, ax=ax, feature_names=data_clf_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7946ea",
   "metadata": {},
   "source": [
    "> Como se esperaba, la rama izquierda del árbol continúa creciendo, mientras que en la rama derecha no se hacen más divisiones.\n",
    "- Fijar `max_depth` cortaría el árbol horizontalmente a un nivel específico, ya sea que estofuera más beneficioso o no.\n",
    "\n",
    "<br>\n",
    "\n",
    "Los hiperparámetros **`min_samples_leaf`, `min_samples_split`, `max_leaf_nodes` o `min_impurity_decrease`** permiten hacer crecer árboles asimétricos y aplicar una restricción en el nivel de hojas o nodos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b5bb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples_leaf = 60\n",
    "tree_clf = DecisionTreeClassifier(min_samples_leaf=min_samples_leaf)\n",
    "fit_and_plot_classification(\n",
    "    tree_clf, data_clf, data_clf_columns, target_clf_column)\n",
    "_ = plt.title(\n",
    "    f\"Árbol de decisión con hoja al menos de {min_samples_leaf} muestras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adfcd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(10, 7))\n",
    "_ = plot_tree(tree_clf, ax=ax, feature_names=data_clf_columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
