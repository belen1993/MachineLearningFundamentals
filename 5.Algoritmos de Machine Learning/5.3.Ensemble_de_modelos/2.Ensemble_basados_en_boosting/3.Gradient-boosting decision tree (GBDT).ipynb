{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "114a6dbd",
   "metadata": {},
   "source": [
    "# Gradient-boosting decision tree (GBDT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccef56f",
   "metadata": {},
   "source": [
    "El Gradient-boosting difiere de Adaboost en lo siguiente: \n",
    "- En lugar de asignar pesos a muestras específicas, GBDT se ajustará a un árbol de decisión sobre el error de residuos (de ahí el nombre de \"gradiente\") del árbol anterior. \n",
    "- Por tanto, cada árbol nuevo en el conjunto predice el error cometido por el modelo anterior en lugar de predecir el objetivo directamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c838814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Cree un generador de números aleatorios\n",
    "rng = np.random.RandomState(0)\n",
    "\n",
    "\n",
    "def generate_data(n_samples=50):\n",
    "    \"\"\"Generate synthetic dataset. Returns `data_train`, `data_test`,\n",
    "    `target_train`.\"\"\"\n",
    "    x_max, x_min = 1.4, -1.4\n",
    "    len_x = x_max - x_min\n",
    "    x = rng.rand(n_samples) * len_x - len_x / 2\n",
    "    noise = rng.randn(n_samples) * 0.3\n",
    "    y = x ** 3 - 0.5 * x ** 2 + noise\n",
    "\n",
    "    data_train = pd.DataFrame(x, columns=[\"Feature\"])\n",
    "    data_test = pd.DataFrame(np.linspace(x_max, x_min, num=300),\n",
    "                             columns=[\"Feature\"])\n",
    "    target_train = pd.Series(y, name=\"Target\")\n",
    "\n",
    "    return data_train, data_test, target_train\n",
    "\n",
    "\n",
    "data_train, data_test, target_train = generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dce827f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.scatterplot(x=data_train[\"Feature\"], y=target_train, color=\"black\",\n",
    "                alpha=0.5)\n",
    "_ = plt.title(\"Synthetic regression dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1274d513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un regresor de árbol de decisión. \n",
    "# Establecemos la profundidad del árbol para que el modelo resultante no subajuste a los datos.\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree = DecisionTreeRegressor(max_depth=3, random_state=0)\n",
    "tree.fit(data_train, target_train)\n",
    "\n",
    "target_train_predicted = tree.predict(data_train)\n",
    "target_test_predicted = tree.predict(data_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c38e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grafica los datos\n",
    "sns.scatterplot(x=data_train[\"Feature\"], y=target_train, color=\"black\",\n",
    "                alpha=0.5)\n",
    "# grafica las predicciones\n",
    "line_predictions = plt.plot(data_test[\"Feature\"], target_test_predicted, \"--\")\n",
    "\n",
    "# grafica los residuos\n",
    "for value, true, predicted in zip(data_train[\"Feature\"],\n",
    "                                  target_train,\n",
    "                                  target_train_predicted):\n",
    "    lines_residuals = plt.plot([value, value], [true, predicted], color=\"red\")\n",
    "\n",
    "plt.legend([line_predictions[0], lines_residuals[0]],\n",
    "           [\"Fitted tree\", \"Residuals\"])\n",
    "_ = plt.title(\"Función de predicción junto \\na errores en el conjunto de entrenamiento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacc261c",
   "metadata": {},
   "source": [
    "> Dado que el árbol subajusta los datos, su precisión está lejos de ser perfecta en los datos de entrenamiento.\n",
    "- Podemos observar esto en la figura observando la diferencia entre predicciones y los datos reales.\n",
    "- Representamos estos errores, llamados *\"residuos\"*, con líneas rojas.\n",
    "\n",
    "> En un algoritmo de refuerzo de gradiente, la idea es crear un segundo árbol que, dados los mismos datos, intentará predecir los residuos en lugar del target del vector.\n",
    "- Por tanto, tendremos un árbol que puede predecir los errores cometidos por el árbol inicial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcff0bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos dicho árbol.\n",
    "\n",
    "residuals = target_train - target_train_predicted\n",
    "\n",
    "tree_residuals = DecisionTreeRegressor(max_depth=5, random_state=0)\n",
    "tree_residuals.fit(data_train, residuals)\n",
    "\n",
    "target_train_predicted_residuals = tree_residuals.predict(data_train)\n",
    "target_test_predicted_residuals = tree_residuals.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728d26f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=data_train[\"Feature\"], y=residuals, color=\"black\", alpha=0.5)\n",
    "line_predictions = plt.plot(\n",
    "    data_test[\"Feature\"], target_test_predicted_residuals, \"--\")\n",
    "\n",
    "# los residuos de los residuos predichos\n",
    "for value, true, predicted in zip(data_train[\"Feature\"],\n",
    "                                  residuals,\n",
    "                                  target_train_predicted_residuals):\n",
    "    lines_residuals = plt.plot([value, value], [true, predicted], color=\"red\")\n",
    "\n",
    "plt.legend([line_predictions[0], lines_residuals[0]],\n",
    "           [\"Fitted tree\", \"Residuals\"], bbox_to_anchor=(1.05, 0.8),\n",
    "           loc=\"upper left\")\n",
    "_ = plt.title(\"Predicción de los residuos anteriores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed202a2c",
   "metadata": {},
   "source": [
    "> Vemos que este nuevo árbol solo logra que se ajusten alguno de los residuos.\n",
    "- Nos centraremos en una muestra específica del conjunto de entrenamiento (es decir, sabemos que la muestra estará bien predicha usando dos árboles sucesivos).\n",
    "- Usaremos esta muestra para explicar cómo se combinan las predicciones de ambos árboles.\n",
    "\n",
    "Primero seleccionemos esta muestra en data_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bc1c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = data_train.iloc[[-2]]\n",
    "x_sample = sample['Feature'].iloc[0]\n",
    "target_true = target_train.iloc[-2]\n",
    "target_true_residual = residuals.iloc[-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786f48ef",
   "metadata": {},
   "source": [
    "**Graficamos la información anterior y resaltemos nuestra muestra de interés.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbf471b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graficar los datos originales y la predicción del primer árbol de decisión.\n",
    "\n",
    "# graficar la información anterior:\n",
    "#   * el conjunto de datos\n",
    "#   * las predicciones\n",
    "#   * los residuos\n",
    "\n",
    "sns.scatterplot(x=data_train[\"Feature\"], y=target_train, color=\"black\",\n",
    "                alpha=0.5)\n",
    "plt.plot(data_test[\"Feature\"], target_test_predicted, \"--\")\n",
    "for value, true, predicted in zip(data_train[\"Feature\"],\n",
    "                                  target_train,\n",
    "                                  target_train_predicted):\n",
    "    lines_residuals = plt.plot([value, value], [true, predicted], color=\"red\")\n",
    "\n",
    "# Resaltar la muestra de interés\n",
    "plt.scatter(sample, target_true, label=\"Muestra de interés\",\n",
    "            color=\"tab:orange\", s=200)\n",
    "plt.xlim([-1, 0])\n",
    "plt.legend(bbox_to_anchor=(1.05, 0.8), loc=\"upper left\")\n",
    "_ = plt.title(\"Tree predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91940d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora, graficamos la información de los residuos. \n",
    "# Trazaremos los residuos calculados a partir del primer árbol de decisión y mostraremos las predicciones residuales.\n",
    "\n",
    "# graficar la información anterior:\n",
    "# * Los residuos generados por el primer árbol\n",
    "# * Las predicciones residuales\n",
    "# * Los residuos de las predicciones residuales\n",
    "\n",
    "sns.scatterplot(x=data_train[\"Feature\"], y=residuals,\n",
    "                color=\"black\", alpha=0.5)\n",
    "plt.plot(data_test[\"Feature\"], target_test_predicted_residuals, \"--\")\n",
    "for value, true, predicted in zip(data_train[\"Feature\"],\n",
    "                                  residuals,\n",
    "                                  target_train_predicted_residuals):\n",
    "    lines_residuals = plt.plot([value, value], [true, predicted], color=\"red\")\n",
    "\n",
    "# Resaltar la muestra de interés\n",
    "plt.scatter(sample, target_true_residual, label=\"Muestra de interés\",\n",
    "            color=\"tab:orange\", s=200)\n",
    "plt.xlim([-1, 0])\n",
    "plt.legend()\n",
    "_ = plt.title(\"Predicción de los residuos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c86915",
   "metadata": {},
   "source": [
    "- Para nuestra muestra de interés, nuestro árbol inicial está cometiendo un error (pequeño residuo).\n",
    "- Al ajustar el segundo árbol, el residuo en este caso está perfectamente ajustado y predicho.\n",
    "- Verificamos cuantitativamente esta predicción usando el árbol ajustado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521d14ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero, verifiquemos la predicción del árbol inicial y comparemos con el valor real.\n",
    "\n",
    "print(f\"Valor real a predecir para \"\n",
    "      f\"f(x={x_sample:.3f}) = {target_true:.3f}\")\n",
    "\n",
    "y_pred_first_tree = tree.predict(sample)[0]\n",
    "print(f\"Predicción del primer árbol de decisión para x={x_sample:.3f}: \"\n",
    "      f\"y={y_pred_first_tree:.3f}\")\n",
    "print(f\"Error del árbol: {target_true - y_pred_first_tree:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef27db32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como observamos visualmente, tenemos un pequeño error. \n",
    "# Ahora, podemos usar el segundo árbol para tratar de predecir este residual.\n",
    "\n",
    "print(f\"Predicción de la residuo para x={x_sample:.3f}: \"\n",
    "      f\"{tree_residuals.predict(sample)[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4070d92a",
   "metadata": {},
   "source": [
    "- Vemos que nuestro segundo árbol es capaz de predecir el residuo exacto (error) de nuestro primer árbol.\n",
    "- Por lo tanto, podemos predecir el valor de x sumando la predicción de todos los árboles en el conjunto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a3e907",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_first_and_second_tree = (\n",
    "    y_pred_first_tree + tree_residuals.predict(sample)[0]\n",
    ")\n",
    "print(f\"Predicción del primer y segundo árbol de decisión combinados para \"\n",
    "      f\"x={x_sample:.3f}: y={y_pred_first_and_second_tree:.3f}\")\n",
    "print(f\"Error del árbol: {target_true - y_pred_first_and_second_tree:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2760b376",
   "metadata": {},
   "source": [
    "- Elegimos una muestra para la cual solo dos árboles eran suficientes para hacer la predicción perfecta.\n",
    "- Sin embargo, vimos en la gráfica anterior que dos árboles no eran suficientes para corregir los residuos de todas las muestras.\n",
    "- Por lo tanto, se debe agregar varios árboles al conjunto para corregir con éxito el error (es decir, el segundo árbol corrige el error del primer árbol; el tercer árbol corrige el error del segundo árbol, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a657a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compararemos el rendimiento de generalización del random-forest y el gradient boosting en el conjunto de datos de viviendas de California.\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "data, target = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "target *= 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53c35ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gradient_boosting = GradientBoostingRegressor(n_estimators=200)\n",
    "cv_results_gbdt = cross_validate(\n",
    "    gradient_boosting, data, target, scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df3fb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gradient Boosting Decision Tree\")\n",
    "print(f\"Error absoluto medio por validación cruzada: \"\n",
    "      f\"{-cv_results_gbdt['test_score'].mean():.3f} ± \"\n",
    "      f\"{cv_results_gbdt['test_score'].std():.3f} k$\")\n",
    "print(f\"Tiempo de ajuste promedio: \"\n",
    "      f\"{cv_results_gbdt['fit_time'].mean():.3f} seconds\")\n",
    "print(f\"Tiempo de puntaje promedio: \"\n",
    "      f\"{cv_results_gbdt['score_time'].mean():.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9a44fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "random_forest = RandomForestRegressor(n_estimators=200, n_jobs=2)\n",
    "cv_results_rf = cross_validate(\n",
    "    random_forest, data, target, scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399acf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random Forest\")\n",
    "print(f\"Error absoluto medio por validación cruzada: \"\n",
    "      f\"{-cv_results_rf['test_score'].mean():.3f} ± \"\n",
    "      f\"{cv_results_rf['test_score'].std():.3f} k$\")\n",
    "print(f\"Tiempo de ajuste promedio: \"\n",
    "      f\"{cv_results_rf['fit_time'].mean():.3f} seconds\")\n",
    "print(f\"Tiempo de puntaje promedio: \"\n",
    "      f\"{cv_results_rf['score_time'].mean():.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fc5093",
   "metadata": {},
   "source": [
    "> - En términos de **rendimiento de cálculo, el bosque puede ser paralelizado** y se beneficiará al usar múltiples núcleos de la CPU.\n",
    "> - En términos de **rendimiento de puntuación, ambos algoritmos conducen a resultados muy cercanos**.\n",
    "> - Sin embargo, vemos que el **gradient boosting es muy rápido** en comparación con el bosque aleatorio. Debido al hecho de que el gradient boosting usa árboles poco profundos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
