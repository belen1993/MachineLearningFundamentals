{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1acc4631",
   "metadata": {},
   "source": [
    "# Adaptive Boosting - AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222f2a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predeciremos las especies de pingüinos de la longitud y profundidad de culmen.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "penguins = pd.read_csv(\"../../../data/penguins/penguins_classification.csv\")\n",
    "culmen_columns = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]\n",
    "target_column = \"Species\"\n",
    "\n",
    "data, target = penguins[culmen_columns], penguins[target_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd5311a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos un árbol de decisión poco profundo.\n",
    "# Since it is shallow, it is unlikely to overfit and some of the training examples will even be misclassified.\n",
    "# Dado que es superficial, es poco probable que sobrejuste y e inclusi algunas muestras de entrenamiento clasificarán erróneamente.\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "palette = [\"tab:red\", \"tab:blue\", \"black\"]\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=2, random_state=0)\n",
    "tree.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f152da7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# podemos predecir en el mismo conjunto de datos y verificar qué muestras se clasifican erróneamente.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "target_predicted = tree.predict(data)\n",
    "misclassified_samples_idx = np.flatnonzero(target != target_predicted)\n",
    "data_misclassified = data.iloc[misclassified_samples_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354b25c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "    tree, data, response_method=\"predict\", cmap=\"RdBu\", alpha=0.5\n",
    ")\n",
    "\n",
    "# el conjunto de datos original\n",
    "sns.scatterplot(data=penguins, x=culmen_columns[0], y=culmen_columns[1],\n",
    "                hue=target_column, palette=palette)\n",
    "# las muestras mal clasificadas\n",
    "sns.scatterplot(data=data_misclassified, x=culmen_columns[0],\n",
    "                y=culmen_columns[1], label=\"Misclassified samples\",\n",
    "                marker=\"+\", s=150, color=\"k\")\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.04, 0.5), loc=\"center left\")\n",
    "_ = plt.title(\"Predicciones de árbol de decisión \\ncon muestras mal clasificadas \"\n",
    "              \"highlighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce9f81c",
   "metadata": {},
   "source": [
    "> Hemos mencionado que el Boosting se basa en crear un nuevo clasificador que intente corregir estas clasificaciones erróneas.\n",
    "- En Scikit-Learn, los modelos tienen un parámetro `sample_weight` que le obliga a prestar más atención a las muestras con pesos más altos durante el entrenamiento.\n",
    "\n",
    "Este parámetro se establece al llamar a `classifier.fit(X, y, sample_weight=weights)`.\n",
    "- Usaremos este truco para crear un nuevo clasificador \"descartando\" todas las muestras clasificadas correctamente y solo considerando las muestras mal clasificadas.\n",
    "- Se asignará un peso de 1 a las muestras mal clasificadas  y 0 a las muestras bien clasificadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4ad31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weight = np.zeros_like(target, dtype=int)\n",
    "sample_weight[misclassified_samples_idx] = 1\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=2, random_state=0)\n",
    "tree.fit(data, target, sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153f1a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "DecisionBoundaryDisplay.from_estimator(\n",
    "    tree, data, response_method=\"predict\", cmap=\"RdBu\", alpha=0.5\n",
    ")\n",
    "sns.scatterplot(data=penguins, x=culmen_columns[0], y=culmen_columns[1],\n",
    "                hue=target_column, palette=palette)\n",
    "sns.scatterplot(data=data_misclassified, x=culmen_columns[0],\n",
    "                y=culmen_columns[1],\n",
    "                label=\"Muestras previamente mal clasificadas\",\n",
    "                marker=\"+\", s=150, color=\"k\")\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.04, 0.5), loc=\"center left\")\n",
    "_ = plt.title(\"Árbol de decisión cambiando los pesos de muestra\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2d8104",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_predicted = tree.predict(data)\n",
    "newly_misclassified_samples_idx = np.flatnonzero(target != target_predicted)\n",
    "remaining_misclassified_samples_idx = np.intersect1d(\n",
    "    misclassified_samples_idx, newly_misclassified_samples_idx\n",
    ")\n",
    "\n",
    "print(f\"Número de muestras previamente mal clasificadas y \"\n",
    "      f\"aún mal clasificado: {len(remaining_misclassified_samples_idx)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb169d0e",
   "metadata": {},
   "source": [
    "> Sin embargo, estamos cometiendo errores en muestras previamente bien clasificadas.\n",
    "- Por tanto, obtenemos entendemos que debemos ponderar las predicciones de cada clasificador de manera diferente, \n",
    "- muy probablemente usando el número de errores que cada clasificador está cometiendo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7634cf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podríamos usar el error de clasificación para combinar ambos árboles.\n",
    "\n",
    "ensemble_weight = [\n",
    "    (target.shape[0] - len(misclassified_samples_idx)) / target.shape[0],\n",
    "    (target.shape[0] - len(newly_misclassified_samples_idx)) / target.shape[0],\n",
    "]\n",
    "ensemble_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2868a7",
   "metadata": {},
   "source": [
    "> El primer clasificador fue 94% preciso y el segundo 69%.\n",
    "- Por tanto, al predecir una clase, debemos confiar en el primer clasificador un poco más que el segundo.\n",
    "- Podríamos usar estos valores de precisión para ponderar las predicciones de cada modelo.\n",
    "\n",
    "> El Boosting requiere una estrategia para combinar los modelos:\n",
    "- Se necesita definir una forma de calcular los pesos que se asignarán a las muestras;\n",
    "- Se debe asignar un peso a cada modelo al hacer predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0366e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usaremos el clasificador AdaBoost y analizaremos los clasificadores de árbol de decisión subyacentes.\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "base_estimator = DecisionTreeClassifier(max_depth=3, random_state=0)\n",
    "adaboost = AdaBoostClassifier(estimator=base_estimator,\n",
    "                              n_estimators=3, algorithm=\"SAMME\",\n",
    "                              random_state=0)\n",
    "adaboost.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed18a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "for boosting_round, tree in enumerate(adaboost.estimators_):\n",
    "    plt.figure()\n",
    "    # Convertimos `data` en una matriz numpy\n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        tree, data.to_numpy(), response_method=\"predict\", cmap=\"RdBu\", alpha=0.5\n",
    "    )\n",
    "    sns.scatterplot(x=culmen_columns[0], y=culmen_columns[1],\n",
    "                    hue=target_column, data=penguins,\n",
    "                    palette=palette)\n",
    "    plt.legend(bbox_to_anchor=(1.04, 0.5), loc=\"center left\")\n",
    "    _ = plt.title(f\"Árbol de decisión entrenado en ronda {boosting_round}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a909e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Peso de cada clasificador: {adaboost.estimator_weights_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0670081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Error de cada clasificador: {adaboost.estimator_errors_}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
