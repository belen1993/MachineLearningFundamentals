{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55461e3c",
   "metadata": {},
   "source": [
    "# Acelerando el gradient-boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5541e70a",
   "metadata": {},
   "source": [
    "En gradient-boosting, el algoritmo es secuencial: requiere que los árboles N-1 hayan sido ajustados para poder ajustar el árbol en la etapa N. \n",
    "- Por tanto, el algoritmo es bastante costoso. \n",
    "- La parte más cara es la búsqueda de la mejor división en el árbol, que es una aproximación de fuerza bruta: se evalúan todas las divisiones posibles y se elige la mejor.\n",
    "\n",
    "Para acelerar el algoritmo, se puede reducir el número de divisiones a evaluar. \n",
    "- el rendimiento de generalización de dicho árbol se reducirá. \n",
    "- sin embargo, dado que estamos combinando varios árboles, podemos agregar más estimadores para superar este problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eae9d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# California housing dataset.\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "data, target = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "target *= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b16712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haga un benchmark rápido del gradient-boosting original.\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gradient_boosting = GradientBoostingRegressor(n_estimators=200)\n",
    "cv_results_gbdt = cross_validate(\n",
    "    gradient_boosting, data, target, scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606bb647",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gradient Boosting Decision Tree\")\n",
    "print(f\"Error absoluto medio por validación cruzada: \"\n",
    "      f\"{-cv_results_gbdt['test_score'].mean():.3f} ± \"\n",
    "      f\"{cv_results_gbdt['test_score'].std():.3f} k$\")\n",
    "print(f\"Tiempo de ajuste promedio: \"\n",
    "      f\"{cv_results_gbdt['fit_time'].mean():.3f} seconds\")\n",
    "print(f\"Tiempo de puntaje promedio: \"\n",
    "      f\"{cv_results_gbdt['score_time'].mean():.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e85cd10",
   "metadata": {},
   "source": [
    "- Recordemos que una forma de acelerar el gradient-boosting es reducir el número de divisiones consideradaas dentro de la agrupación de árboles.\n",
    "- Una forma es agrupar los datos antes de darlo al gradient-boosting.\n",
    "    - El transformador KBinsDiscretizer los hace.\n",
    "    - Podemos generar un pipeline con este preprocesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889152ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos a demostrar la transformación realizada por el KbinsdIsCretizer.\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "discretizer = KBinsDiscretizer(\n",
    "    n_bins=256, encode=\"ordinal\", strategy=\"quantile\")\n",
    "data_trans = discretizer.fit_transform(data)\n",
    "data_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618ee7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cada valor representa el índice de bin cuando se procesa la distribución por cuantil. \n",
    "# Podemos verificar el número de contenedores por caraterística.\n",
    "\n",
    "[len(np.unique(col)) for col in data_trans.T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f48a2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizaremos este transformador para discretizar los datos antes de entrenar al regresor de gradient boosting .\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "gradient_boosting = make_pipeline(\n",
    "    discretizer, GradientBoostingRegressor(n_estimators=200))\n",
    "cv_results_gbdt = cross_validate(\n",
    "    gradient_boosting, data, target, scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44c5549",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gradient Boosting Decision Tree with KBinsDiscretizer\")\n",
    "print(f\"Error absoluto medio por validación cruzada: \"\n",
    "      f\"{-cv_results_gbdt['test_score'].mean():.3f} ± \"\n",
    "      f\"{cv_results_gbdt['test_score'].std():.3f} k$\")\n",
    "print(f\"Tiempo de ajuste promedio: \"\n",
    "      f\"{cv_results_gbdt['fit_time'].mean():.3f} seconds\")\n",
    "print(f\"Tiempo de puntaje promedio: \"\n",
    "      f\"{cv_results_gbdt['score_time'].mean():.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dfb22c",
   "metadata": {},
   "source": [
    "- Scikit-Learn proporciona clases específicas que están aún más optimizadas para un conjunto de datos grande: `HistGradientBoostingClassifier` y `HistGradientBoostingRegressor`.\n",
    "- Cada característica en los datos del dataset se agrupa primero al calcular los histogramas, luego se utilizan para evaluar las divisiones potenciales.\n",
    "- El número de divisiones para evaluar es mucho más pequeño.\n",
    "- Este algoritmo es mucho más eficiente que el gradient boosting cuando el conjunto de datos tiene más de 10,000 muestras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6448234b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "histogram_gradient_boosting = HistGradientBoostingRegressor(\n",
    "    max_iter=200, random_state=0)\n",
    "cv_results_hgbdt = cross_validate(\n",
    "    histogram_gradient_boosting, data, target,\n",
    "    scoring=\"neg_mean_absolute_error\", n_jobs=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4360083f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Árbol de decisión de impulso de gradiente de histograma\")\n",
    "print(f\"Mean absolute error via cross-validation: \"\n",
    "      f\"{-cv_results_hgbdt['test_score'].mean():.3f} ± \"\n",
    "      f\"{cv_results_hgbdt['test_score'].std():.3f} k$\")\n",
    "print(f\"Tiempo de ajuste promedio: \"\n",
    "      f\"{cv_results_hgbdt['fit_time'].mean():.3f} seconds\")\n",
    "print(f\"Tiempo de puntaje promedio: \"\n",
    "      f\"{cv_results_hgbdt['score_time'].mean():.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5422ad7",
   "metadata": {},
   "source": [
    "> - El histograma de gradient-boosting es el mejor algoritmo en términos de puntaje.\n",
    "> - También escalará con el aumento de número de muestras, mientras que el gradient-boosting normal no lo hace."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
