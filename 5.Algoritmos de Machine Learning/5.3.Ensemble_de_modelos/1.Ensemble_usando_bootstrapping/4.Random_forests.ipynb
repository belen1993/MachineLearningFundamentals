{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "117b7cb2",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f497511",
   "metadata": {},
   "source": [
    "Los bosques aleatorios tienen la particularidad de: **al entrenar un árbol, la búsqueda de la mejor división se realiza solo en un subconjunto de las características originales tomadas al azar.**\n",
    "\n",
    "- Los subconjuntos aleatorios son diferentes para cada nodo dividido.\n",
    "- El objetivo es inyectar aleatorización adicional en el procedimiento de aprendizaje para tratar de decorrelacionar los errores de predicción de los árboles individuales.\n",
    "\n",
    "Por lo tanto, los bosques aleatorios están utilizando aleatorización en ambos ejes de la matriz de datos:\n",
    "- Al generar muestras de arranque (boostrapping) para cada árbol en el bosque;\n",
    "- Seleccionando aleatoriamente un subconjunto de características en cada nodo del árbol."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf3a2fa",
   "metadata": {},
   "source": [
    "# Una mirada a los bosques aleatorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d529de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# veremos el uso de un clasificador de bosque aleatorio en el conjunto de datos del censo de adultos.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "adult_census = pd.read_csv(\"../../../data/adult-census-numeric/full.csv\")\n",
    "target_name = \"class\"\n",
    "data = adult_census.drop(columns=[target_name, \"education-num\"])\n",
    "target = adult_census[target_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f65ba9",
   "metadata": {},
   "source": [
    "> El censo de adultos contiene algunos datos categóricos. Los codificamos utilizando OrdinalEncoder, ya que los modelos basados ​​en árboles pueden funcionar de manera muy eficiente con una representación así de variables categóricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "373c3740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "\n",
    "categorical_encoder = OrdinalEncoder(\n",
    "    handle_unknown=\"use_encoded_value\", unknown_value=-1\n",
    ")\n",
    "preprocessor = make_column_transformer(\n",
    "    (categorical_encoder, make_column_selector(dtype_include=object)),\n",
    "    remainder=\"passthrough\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7baeb60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero daremos un ejemplo simple en el que capacitaremos a un solo clasificador de árbol de decisión \n",
    "# y verificaremos su rendimiento de generalización a través de la validación cruzada.\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = make_pipeline(preprocessor, DecisionTreeClassifier(random_state=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a14b7340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree classifier: 0.820 ± 0.006\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores_tree = cross_val_score(tree, data, target)\n",
    "\n",
    "print(f\"Decision tree classifier: \"\n",
    "      f\"{scores_tree.mean():.3f} ± {scores_tree.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed62f9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construimos un BaggingClassifier con un clasificador de árbol de decisión como modelo base.\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bagged_trees = make_pipeline(\n",
    "    preprocessor,\n",
    "    BaggingClassifier(\n",
    "        estimator=DecisionTreeClassifier(random_state=0),\n",
    "        n_estimators=50, n_jobs=2, random_state=0,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23d8f3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagged decision tree classifier: 0.846 ± 0.005\n"
     ]
    }
   ],
   "source": [
    "scores_bagged_trees = cross_val_score(bagged_trees, data, target)\n",
    "\n",
    "print(f\"Bagged decision tree classifier: \"\n",
    "      f\"{scores_bagged_trees.mean():.3f} ± {scores_bagged_trees.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b44531c",
   "metadata": {},
   "source": [
    "Ahora, usaremos un **bosque aleatorio**.\n",
    "> Observar que no necesitamos especificar ningún base_estimator porque el estimador se ve obligado a ser un árbol de decisión.\n",
    "- Solo especificamos el número deseado de árboles en el bosque.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef30e6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest = make_pipeline(\n",
    "    preprocessor,\n",
    "    RandomForestClassifier(n_estimators=50, n_jobs=2, random_state=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "caa64bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest classifier: 0.851 ± 0.004\n"
     ]
    }
   ],
   "source": [
    "scores_random_forest = cross_val_score(random_forest, data, target)\n",
    "\n",
    "print(f\"Random forest classifier: \"\n",
    "      f\"{scores_random_forest.mean():.3f} ± \"\n",
    "      f\"{scores_random_forest.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3e61ba",
   "metadata": {},
   "source": [
    "# Detalles sobre hiperparámetros predeterminados\n",
    "Para los bosques aleatorios, es posible controlar la cantidad de aleatoriedad para cada división estableciendo el valor del hiperparámetro `max_features`:\n",
    "- max_features=0.5 significa que el 50% de las características se deben considerar en cada división;\n",
    "- max_features=1.0 significa que todas las características se deben considerar en cada división, lo que desactiva efectivamente el submuestreo de características.\n",
    "\n",
    "De manera predeterminada, `RandomForestRegressor` deshabilita el submuestreo de características, mientras que `RandomForestClassifier` usa `max_features = np.sqrt(n_features)`.\n",
    "- Estos valores predeterminados reflejan buenas prácticas dadas en la literatura científica.\n",
    "\n",
    "> Sin embargo, max_Feature es uno de los **hiperparámetros a considerar** al ajustar un bosque aleatorio:\n",
    ">- **demasiada aleatoriedad** en los árboles puede conducir a modelos base subajustados y puede ser perjudicial para la agrupación en su conjunto,\n",
    ">- **muy poca aleatoriedad** en los árboles conduce a una mayor correlación de los errores de predicción y, como resultado, reduce los beneficios del paso de promediado en términos de control de sobreajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d91569",
   "metadata": {},
   "source": [
    "**En la siguiente tabla se resumen estos detalles:**\n",
    "<table class=\"colwidths-auto table\">\n",
    "<thead>\n",
    "<tr class=\"row-odd\"><th class=\"head\"><p>Ensemble model class</p></th>\n",
    "<th class=\"head\"><p>Base model class</p></th>\n",
    "<th class=\"head\"><p>Default value for <code class=\"docutils literal notranslate\"><span class=\"pre\">max_features</span></code></p></th>\n",
    "<th class=\"head\"><p>Features subsampling strategy</p></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr class=\"row-even\"><td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">BaggingClassifier</span></code></p></td>\n",
    "<td><p>User specified (flexible)</p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">n_features</span></code> (no&nbsp;subsampling)</p></td>\n",
    "<td><p>Model level</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">RandomForestClassifier</span></code></p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">DecisionTreeClassifier</span></code></p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">sqrt(n_features)</span></code></p></td>\n",
    "<td><p>Tree node level</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">BaggingRegressor</span></code></p></td>\n",
    "<td><p>User specified (flexible)</p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">n_features</span></code> (no&nbsp;subsampling)</p></td>\n",
    "<td><p>Model level</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">RandomForestRegressor</span></code></p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">DecisionTreeRegressor</span></code></p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">n_features</span></code> (no&nbsp;subsampling)</p></td>\n",
    "<td><p>Tree node level</p></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
