{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54e67f33",
   "metadata": {},
   "source": [
    "# Pipeline Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5779ef10",
   "metadata": {},
   "source": [
    "**“Bagging” viene de Bootstrap AGGregatING.**\n",
    "\n",
    "Usa el remuestreo de bootstrap (muestreo aleatorio con reemplazo) para aprender varios modelos sobre variaciones aleatorias del conjunto de entrenamiento.\n",
    "En el momento de predicción, las predicciones de cada modelo se agregan para dar las predicciones finales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379d6bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generaremos un conjunto de datos sintético simple para obtener información sobre el arranque (bootstraping).\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# cCrea un generador de números aleatorios que se utilizará para establecer la aleatoriedad\n",
    "rng = np.random.RandomState(1)\n",
    "\n",
    "\n",
    "def generate_data(n_samples=30):\n",
    "    \"\"\"Generate synthetic dataset. Returns `data_train`, `data_test`,\n",
    "    `target_train`.\"\"\"\n",
    "    x_min, x_max = -3, 3\n",
    "    x = rng.uniform(x_min, x_max, size=n_samples)\n",
    "    noise = 4.0 * rng.randn(n_samples)\n",
    "    y = x ** 3 - 0.5 * (x + 1) ** 2 + noise\n",
    "    y /= y.std()\n",
    "\n",
    "    data_train = pd.DataFrame(x, columns=[\"Feature\"])\n",
    "    data_test = pd.DataFrame(\n",
    "        np.linspace(x_max, x_min, num=300), columns=[\"Feature\"])\n",
    "    target_train = pd.Series(y, name=\"Target\")\n",
    "\n",
    "    return data_train, data_test, target_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3e15e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "data_train, data_test, target_train = generate_data(n_samples=30)\n",
    "sns.scatterplot(x=data_train[\"Feature\"], y=target_train, color=\"black\",\n",
    "                alpha=0.5)\n",
    "_ = plt.title(\"Synthetic regression dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3526a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La relación entre nuestra característica y el objetivo a predecir no es lineal. \n",
    "# Sin embargo, un árbol de decisión es capaz de aproximar dicha dependencia no lineal:\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree = DecisionTreeRegressor(max_depth=3, random_state=0)\n",
    "tree.fit(data_train, target_train)\n",
    "y_pred = tree.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe51c899",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=data_train[\"Feature\"], y=target_train, color=\"black\",\n",
    "                alpha=0.5)\n",
    "plt.plot(data_test[\"Feature\"], y_pred, label=\"Fitted tree\")\n",
    "plt.legend()\n",
    "_ = plt.title(\"Predictions by a single decision tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ae345c",
   "metadata": {},
   "source": [
    "# Bootstrap resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2119a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crearemos una función que, dados los datos y el target, devolverá una variación remuestreada: data_bootstrap y target_bootstrap.\n",
    "\n",
    "def bootstrap_sample(data, target):\n",
    "    # Indices corresponding to a sampling with replacement of the same sample\n",
    "    # size than the original data\n",
    "    bootstrap_indices = rng.choice(\n",
    "        np.arange(target.shape[0]), size=target.shape[0], replace=True,\n",
    "    )\n",
    "    # In pandas, we need to use `.iloc` to extract rows using an integer\n",
    "    # position index:\n",
    "    data_bootstrap = data.iloc[bootstrap_indices]\n",
    "    target_bootstrap = target.iloc[bootstrap_indices]\n",
    "    return data_bootstrap, target_bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e289e72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generaremos 3 muestras de bootstrap y verificaremos cualitativamente la diferencia con el conjunto de datos original.\n",
    "\n",
    "n_bootstraps = 3\n",
    "for bootstrap_idx in range(n_bootstraps):\n",
    "    # draw a bootstrap from the original data\n",
    "    data_bootstrap, target_bootstrap = bootstrap_sample(\n",
    "        data_train, target_train,\n",
    "    )\n",
    "    plt.figure()\n",
    "    plt.scatter(data_bootstrap[\"Feature\"], target_bootstrap,\n",
    "                color=\"tab:blue\", facecolors=\"none\",\n",
    "                alpha=0.5, label=\"Resampled data\", s=180, linewidth=5)\n",
    "    plt.scatter(data_train[\"Feature\"], target_train,\n",
    "                color=\"black\", s=60,\n",
    "                alpha=1, label=\"Original data\")\n",
    "    plt.title(f\"Resampled data #{bootstrap_idx}\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018025f6",
   "metadata": {},
   "source": [
    "> Las 3 muestras de bootstrap generadas son todas diferentes del conjunto de datos original y entre sí.\n",
    "Para confirmar esto, podemos verificar el número de muestras únicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6b1b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_huge, data_test_huge, target_train_huge = generate_data(\n",
    "    n_samples=100_000)\n",
    "data_bootstrap_sample, target_bootstrap_sample = bootstrap_sample(\n",
    "    data_train_huge, target_train_huge)\n",
    "\n",
    "ratio_unique_sample = (np.unique(data_bootstrap_sample).size /\n",
    "                       data_bootstrap_sample.size)\n",
    "print(\n",
    "    f\"Percentage of samples present in the original dataset: \"\n",
    "    f\"{ratio_unique_sample * 100:.1f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a05001",
   "metadata": {},
   "source": [
    "> Usando bootstrap podemos generar muchos datasets, todos ligeramente diferentes.\n",
    "- Podemos ajustar un árbol de decisión a cada uno de estos conjuntos de datos y todos a su vez serán ligeramente diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1239e720",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_trees = []\n",
    "for bootstrap_idx in range(n_bootstraps):\n",
    "    tree = DecisionTreeRegressor(max_depth=3, random_state=0)\n",
    "\n",
    "    data_bootstrap_sample, target_bootstrap_sample = bootstrap_sample(\n",
    "        data_train, target_train)\n",
    "    tree.fit(data_bootstrap_sample, target_bootstrap_sample)\n",
    "    bag_of_trees.append(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6303093d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora que hemos creado una bolsa de diferentes árboles, podemos usar cada uno de los árboles para predecir las muestras dentro del rango de datos.\n",
    "# Darán predicciones ligeramente diferentes.\n",
    "\n",
    "sns.scatterplot(x=data_train[\"Feature\"], y=target_train, color=\"black\",\n",
    "                alpha=0.5)\n",
    "for tree_idx, tree in enumerate(bag_of_trees):\n",
    "    tree_predictions = tree.predict(data_test)\n",
    "    plt.plot(data_test[\"Feature\"], tree_predictions, linestyle=\"--\", alpha=0.8,\n",
    "             label=f\"Tree #{tree_idx} predictions\")\n",
    "\n",
    "plt.legend()\n",
    "_ = plt.title(\"Predicciones de árboles entrenados en diferentes bootstraps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e6c0cb",
   "metadata": {},
   "source": [
    "# Agregación\n",
    "\n",
    "- Una vez que nuestros árboles están ajujstados, podemos obtener predicciones para cada uno de ellos.\n",
    "- En la regresión, la forma más directa de combinar esas predicciones es solo promediarlas: \n",
    "    - para un punto de prueba dado, alimentamos los valores de la función de entrada a cada uno de los N modelos entrenados en el conjunto y, \n",
    "    - como resultado, calcula los valores predichos para la variable objetivo.\n",
    "- La predicción final del conjunto para el punto de datos de la prueba es el promedio de esos valores de N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ff7f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graficamos las predicciones promedio del ejemplo anterior.\n",
    "\n",
    "sns.scatterplot(x=data_train[\"Feature\"], y=target_train, color=\"black\",\n",
    "                alpha=0.5)\n",
    "\n",
    "bag_predictions = []\n",
    "for tree_idx, tree in enumerate(bag_of_trees):\n",
    "    tree_predictions = tree.predict(data_test)\n",
    "    plt.plot(data_test[\"Feature\"], tree_predictions, linestyle=\"--\", alpha=0.8,\n",
    "             label=f\"Tree #{tree_idx} predictions\")\n",
    "    bag_predictions.append(tree_predictions)\n",
    "\n",
    "bag_predictions = np.mean(bag_predictions, axis=0)\n",
    "plt.plot(data_test[\"Feature\"], bag_predictions, label=\"Averaged predictions\",\n",
    "         linestyle=\"-\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 0.8), loc=\"upper left\")\n",
    "_ = plt.title(\"Predictions of bagged trees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf318f2",
   "metadata": {},
   "source": [
    "# Bagging en scikit-learn\n",
    "Scikit-Learn implementa el procedimientocomo un **“meta-estimador”**, es decir, un estimador que envuelve otro estimador: \n",
    "- se necesita un modelo base que se clone varias veces y se entrene independientemente en cada muestra de arranque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcdd976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establecemo n_estimators = 100 para obtener un efecto de suavizado más fuerte.\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "bagged_trees = BaggingRegressor(\n",
    "    estimator=DecisionTreeRegressor(max_depth=3),\n",
    "    n_estimators=100,\n",
    ")\n",
    "_ = bagged_trees.fit(data_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd4efa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamoz las predicciones del conjunto en el mismo intervalo de datos:\n",
    "\n",
    "sns.scatterplot(x=data_train[\"Feature\"], y=target_train, color=\"black\",\n",
    "                alpha=0.5)\n",
    "\n",
    "bagged_trees_predictions = bagged_trees.predict(data_test)\n",
    "plt.plot(data_test[\"Feature\"], bagged_trees_predictions)\n",
    "\n",
    "_ = plt.title(\"Predictions from a bagging classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beb5750",
   "metadata": {},
   "source": [
    "> Es posible acceder a los modelos internos del conjunto almacenado como una lista de Python en el atributo `bagged_trees.estimators_` después del ajuste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a00dc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparemos las predicciones basadas en el modelo con sus promedios:\n",
    "\n",
    "for tree_idx, tree in enumerate(bagged_trees.estimators_):\n",
    "    label = \"Predictions of individual trees\" if tree_idx == 0 else None\n",
    "    # we convert `data_test` into a NumPy array to avoid a warning raised in scikit-learn\n",
    "    tree_predictions = tree.predict(data_test.to_numpy())\n",
    "    plt.plot(data_test[\"Feature\"], tree_predictions, linestyle=\"--\", alpha=0.1,\n",
    "             color=\"tab:blue\", label=label)\n",
    "\n",
    "sns.scatterplot(x=data_train[\"Feature\"], y=target_train, color=\"black\",\n",
    "                alpha=0.5)\n",
    "\n",
    "bagged_trees_predictions = bagged_trees.predict(data_test)\n",
    "plt.plot(data_test[\"Feature\"], bagged_trees_predictions,\n",
    "         color=\"tab:orange\", label=\"Predictions of ensemble\")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a546389",
   "metadata": {},
   "source": [
    "# Pipeline complejo\n",
    "- Si bien utilizamos un árbol de decisión como modelo base, **nada nos impide usar otro tipo de modelo**.\n",
    "\n",
    "- Como sabemos que la función de generación de datos original es una transformación polinómica ruidosa de la variable de entrada, intentemos ajustar un pipeline de regresión polinómica en este conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afa1681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "polynomial_regressor = make_pipeline(\n",
    "    MinMaxScaler(),\n",
    "    PolynomialFeatures(degree=4),\n",
    "    Ridge(alpha=1e-10),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b794e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# El conjunto en sí se construye simplemente pasando el pipeline como parámetro `estimator` de la clase BaggingRegresor:\n",
    "\n",
    "bagging = BaggingRegressor(\n",
    "    estimator=polynomial_regressor,\n",
    "    n_estimators=100,\n",
    "    random_state=0,\n",
    ")\n",
    "_ = bagging.fit(data_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fa146e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, regressor in enumerate(bagging.estimators_):\n",
    "    # Convertimos `data_test` en una matriz numpy para evitar una advertencia planteada en Scikit-Learn\n",
    "    regressor_predictions = regressor.predict(data_test.to_numpy())\n",
    "    base_model_line = plt.plot(\n",
    "        data_test[\"Feature\"], regressor_predictions, linestyle=\"--\", alpha=0.2,\n",
    "        label=\"Predictions of base models\" if i == 0 else None,\n",
    "        color=\"tab:blue\"\n",
    "    )\n",
    "\n",
    "sns.scatterplot(x=data_train[\"Feature\"], y=target_train, color=\"black\",\n",
    "                alpha=0.5)\n",
    "bagging_predictions = bagging.predict(data_test)\n",
    "plt.plot(data_test[\"Feature\"], bagging_predictions,\n",
    "         color=\"tab:orange\", label=\"Predictions of ensemble\")\n",
    "plt.ylim(target_train.min(), target_train.max())\n",
    "plt.legend()\n",
    "_ = plt.title(\"Bagged polynomial regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b946986",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
