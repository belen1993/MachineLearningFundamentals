{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13fbc9a5",
   "metadata": {},
   "source": [
    "# Ajuste de hiperparametros con agrupamientos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea2d91d",
   "metadata": {},
   "source": [
    "# Random forest\n",
    "\n",
    "> **El parámetro principal a sintonizar un bosque aleatorio es `n_estimators`.** \\\n",
    "- En general, cuantos más árboles en el bosque, mejor será el rendimiento de la generalización.\n",
    "- Sin embargo, ralentizará el tiempo de ajuste y predicción.\n",
    "- El objetivo es equilibrar el tiempo de computación y el rendimiento de la generalización al establecer el número de estimadores al poner dicho modelo en producción.\n",
    "\n",
    "> Por tanto, también podríamos **sintonizar un parámetro que controla la profundidad de cada árbol en el bosque.**\n",
    "- Dos parámetros son importantes para esto: `max_depth` y `max_leaf_nodes`.\n",
    "- Difieren en la forma en que controlan la estructura del árbol.\n",
    "- De hecho, max_depth reforzará tener un árbol más simétrico, mientras que max_leaf_nodes no impone dicha restricción."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f4a25c",
   "metadata": {},
   "source": [
    "> ⚠ Tener cuidado con el hecho de que en un bosque aleatorio, los árboles son generalmente profundos ya que buscamos sobreajustar cada árbol en cada muestra en cada boostrap, ya que esto se mitigará combinándolos todos.\n",
    "> - Agrupando de árboles subajustados (es decir, árboles poco profundos) también puede conducir a un bosque subajustado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698456af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data, target = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "target *= 100\n",
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    data, target, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0839a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "param_distributions = {\n",
    "    \"n_estimators\": [1, 2, 5, 10, 20, 50, 100, 200, 500],\n",
    "    \"max_leaf_nodes\": [2, 5, 10, 20, 50, 100],\n",
    "}\n",
    "search_cv = RandomizedSearchCV(\n",
    "    RandomForestRegressor(n_jobs=2), param_distributions=param_distributions,\n",
    "    scoring=\"neg_mean_absolute_error\", n_iter=10, random_state=0, n_jobs=2,\n",
    ")\n",
    "search_cv.fit(data_train, target_train)\n",
    "\n",
    "columns = [f\"param_{name}\" for name in param_distributions.keys()]\n",
    "columns += [\"mean_test_error\", \"std_test_error\"]\n",
    "cv_results = pd.DataFrame(search_cv.cv_results_)\n",
    "cv_results[\"mean_test_error\"] = -cv_results[\"mean_test_score\"]\n",
    "cv_results[\"std_test_error\"] = cv_results[\"std_test_score\"]\n",
    "cv_results[columns].sort_values(by=\"mean_test_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434c1b6d",
   "metadata": {},
   "source": [
    "- Ahora estimaremos el rendimiento de la generalización del mejor modelo reajustando con el conjunto de capacitación completo y utilizando el conjunto de pruebas para la puntuación en datos no procesados.\n",
    "    - Esto se hace de forma predeterminada al llamar al método `.fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a12bfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "error = -search_cv.score(data_test, target_test)\n",
    "print(f\"En promedio, nuestro regresor de bosque aleatorio comete un error de {error:.2f} k$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d46ee8c",
   "metadata": {},
   "source": [
    "# Gradient-boosting decision trees\n",
    "> Para gradient-boosting **los parámetros están acoplados, por lo que no podemos establecer los parámetros uno tras otro**.\n",
    "- Los parámetros importantes son `n_estimators`, `learning_rate` y, `max_depth` o `max_leaf_nodes` (como se discutió anteriormente en el bosque aleatorio).\n",
    "\n",
    "> **max_depth (o max_leaf_nodes)**: El árbol utilizado en el refuerzo de gradiente debe tener una profundidad baja, típicamente entre 3 y 8 niveles, o pocas hojas (2^3 = 8 a 2^8 = 256).\n",
    "- Tener modelos muy débiles en cada paso ayudará a reducir el sobreajuste.\n",
    "\n",
    "> Con esto en mente, **cuanto más profundo sean los árboles, más rápido se corregirán los residuos y se requerirán menos mdelos**.\n",
    "- Por lo tanto, los `n_estimators` deben aumentarse si `max_depth` es más bajo.\n",
    "\n",
    "> Con una **tasa de aprendizaje muy baja** necesitaremos más estimadores para corregir el error general.\n",
    "- Sin embargo, una **tasa de aprendizaje demasiado grande** tiende a obtener un conjunto sobreajustado, similar a tener una profundidad de árbol demasiado grande."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e883f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import loguniform\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "param_distributions = {\n",
    "    \"n_estimators\": [1, 2, 5, 10, 20, 50, 100, 200, 500],\n",
    "    \"max_leaf_nodes\": [2, 5, 10, 20, 50, 100],\n",
    "    \"learning_rate\": loguniform(0.01, 1),\n",
    "}\n",
    "search_cv = RandomizedSearchCV(\n",
    "    GradientBoostingRegressor(), param_distributions=param_distributions,\n",
    "    scoring=\"neg_mean_absolute_error\", n_iter=20, random_state=0, n_jobs=2\n",
    ")\n",
    "search_cv.fit(data_train, target_train)\n",
    "\n",
    "columns = [f\"param_{name}\" for name in param_distributions.keys()]\n",
    "columns += [\"mean_test_error\", \"std_test_error\"]\n",
    "cv_results = pd.DataFrame(search_cv.cv_results_)\n",
    "cv_results[\"mean_test_error\"] = -cv_results[\"mean_test_score\"]\n",
    "cv_results[\"std_test_error\"] = cv_results[\"std_test_score\"]\n",
    "cv_results[columns].sort_values(by=\"mean_test_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cccb87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we estimate the generalization performance of the best model using the test set.\n",
    "# estimamos el rendimiento de generalización del mejor modelo utilizando el conjunto de pruebas.\n",
    "\n",
    "error = -search_cv.score(data_test, target_test)\n",
    "print(f\"En promedio, nuestro regresor GBDT comete un error de {error:.2f} k$\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba398c1",
   "metadata": {},
   "source": [
    "- La puntuación media de la prueba en el conjunto de pruebas de retención es ligeramente mejor que la puntuación del mejor modelo.\n",
    "- La razón es que el modelo final está reajustado en todo el conjunto de entrenamiento y, por lo tanto, en más datos que los modelos internos cros-validados del procedimiento de búsqueda de cuadrícula."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
