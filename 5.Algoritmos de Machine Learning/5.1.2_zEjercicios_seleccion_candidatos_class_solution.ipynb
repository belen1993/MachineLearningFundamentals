{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Ejercicios de selección de candidatos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clasificación \n",
    "Para el dataset del titanic (data/titanic)\n",
    "- Selecciona los 3 mejores modelos candidatos para este problema de clasificación basándote en su rendimiento (usando las métricas de precisión y F1-score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formato de los prints\n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'\n",
    "\n",
    "def headr(text):\n",
    "    return ('\\n'+color.UNDERLINE + text + color.END+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 12)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carga de datos\n",
    "import pandas as pd\n",
    "\n",
    "titanic = pd.read_csv(\"../data/titanic/train.csv\")\n",
    "target_column = \"Survived\"\n",
    "\n",
    "titanic.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Explorar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.describe(include=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "profile = ProfileReport(titanic, title=\"Titanic Profiling Report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Datos desbalanceados\n",
    "- Datos no normales\n",
    "- Datos no lineales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Limpiar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminar columnas irrelevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass     Sex   Age  SibSp  Parch     Fare Embarked\n",
       "0         0       3    male  22.0      1      0   7.2500        S\n",
       "1         1       1  female  38.0      1      0  71.2833        C\n",
       "2         1       3  female  26.0      0      0   7.9250        S\n",
       "3         1       1  female  35.0      1      0  53.1000        S\n",
       "4         0       3    male  35.0      0      0   8.0500        S"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eliminar columnas irrelevantes\n",
    "\n",
    "discarded_columns = [\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"]\n",
    "titanic_cleaned = titanic.drop(discarded_columns, axis=1)\n",
    "titanic_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_cleaned.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_cleaned.describe(include=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valores faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[4mValores faltantes - original: \u001b[0m\n",
      " Survived      0\n",
      "Pclass        0\n",
      "Sex           0\n",
      "Age         177\n",
      "SibSp         0\n",
      "Parch         0\n",
      "Fare          0\n",
      "Embarked      2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Valores faltantes\n",
    "\n",
    "print(headr(\"Valores faltantes - original: \"), titanic_cleaned.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[4mValores faltantes - imputados: \u001b[0m\n",
      " Survived    0\n",
      "Pclass      0\n",
      "Sex         0\n",
      "Age         0\n",
      "SibSp       0\n",
      "Parch       0\n",
      "Fare        0\n",
      "Embarked    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ricar\\AppData\\Local\\Temp\\ipykernel_27808\\949558213.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  titanic_cleaned[\"Age\"].fillna(titanic_cleaned[\"Age\"].median(), inplace=True)\n",
      "C:\\Users\\ricar\\AppData\\Local\\Temp\\ipykernel_27808\\949558213.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  titanic_cleaned[\"Embarked\"].fillna(titanic_cleaned[\"Embarked\"].mode()[0], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Imputar valores faltantes\n",
    "titanic_cleaned[\"Age\"].fillna(titanic_cleaned[\"Age\"].median(), inplace=True)\n",
    "titanic_cleaned[\"Embarked\"].fillna(titanic_cleaned[\"Embarked\"].mode()[0], inplace=True)\n",
    "\n",
    "print(headr(\"Valores faltantes - imputados: \"), titanic_cleaned.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[4mNumerical columns\u001b[0m\n",
      " ['Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\n",
      "\n",
      "\u001b[4mCategorical columns\u001b[0m\n",
      " ['Sex', 'Embarked']\n"
     ]
    }
   ],
   "source": [
    "# Extraer columnas numéricas y categóricas\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "\n",
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "\n",
    "numerical_columns = numerical_columns_selector(titanic_cleaned)\n",
    "categorical_columns = categorical_columns_selector(titanic_cleaned)\n",
    "\n",
    "print(headr(\"Numerical columns\"), numerical_columns)\n",
    "print(headr(\"Categorical columns\"), categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[4mlower_bound\u001b[0m\n",
      " Survived    -3.0000\n",
      "Pclass      -1.0000\n",
      "Age        -17.0000\n",
      "SibSp       -3.0000\n",
      "Parch        0.0000\n",
      "Fare       -61.3584\n",
      "dtype: float64\n",
      "\n",
      "\u001b[4mupper_bound\u001b[0m\n",
      " Survived      4.0000\n",
      "Pclass        6.0000\n",
      "Age          74.0000\n",
      "SibSp         4.0000\n",
      "Parch         0.0000\n",
      "Fare        100.2688\n",
      "dtype: float64\n",
      "\n",
      "\u001b[4mOutliers:\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Survived      0\n",
       "Pclass        0\n",
       "Age           1\n",
       "SibSp        12\n",
       "Parch       213\n",
       "Fare         53\n",
       "dtype: int64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Outliers\n",
    "\n",
    "# identificación\n",
    "IQR = titanic_cleaned[numerical_columns].quantile(0.75) - titanic_cleaned[numerical_columns].quantile(0.25)\n",
    "lower_bound = titanic_cleaned[numerical_columns].quantile(0.25) - (IQR * 3)\n",
    "upper_bound = titanic_cleaned[numerical_columns].quantile(0.75) + (IQR * 3)\n",
    "\n",
    "print(headr('lower_bound'),lower_bound)\n",
    "print(headr('upper_bound'),upper_bound)\n",
    "\n",
    "outliers = titanic_cleaned[numerical_columns][(titanic_cleaned[numerical_columns] < lower_bound) | (titanic_cleaned[numerical_columns] > upper_bound)]\n",
    "\n",
    "print(headr(\"Outliers:\"))\n",
    "outliers.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age', 'SibSp', 'Parch', 'Fare'], dtype='object')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reemplazo\n",
    "\n",
    "# columnas con outliers\n",
    "columns_with_outliers = outliers.columns[outliers.count() > 0]\n",
    "columns_with_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para winsorizar una columna\n",
    "def winsorize_column(column, lower_bound, upper_bound):\n",
    "    return column.clip(lower=lower_bound, upper=upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[4mOutliers - winsorized:\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Survived    0\n",
       "Pclass      0\n",
       "Age         0\n",
       "SibSp       0\n",
       "Parch       0\n",
       "Fare        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# procesamos todas las comumnas con outliers\n",
    "for col_name in columns_with_outliers:\n",
    "    titanic_cleaned[col_name] = winsorize_column(titanic_cleaned[col_name], lower_bound[col_name], upper_bound[col_name])\n",
    "\n",
    "outliers = titanic_cleaned[numerical_columns][(titanic_cleaned[numerical_columns] < lower_bound) | (titanic_cleaned[numerical_columns] > upper_bound)]\n",
    "\n",
    "print(headr(\"Outliers - winsorized:\"))\n",
    "outliers.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[4mDuplicadas originales:\u001b[0m\n",
      " 0 \n",
      " Empty DataFrame\n",
      "Columns: [PassengerId, Survived, Pclass, Name, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin, Embarked]\n",
      "Index: []\n",
      "\n",
      "\u001b[4mDuplicadas tras limpieza:\u001b[0m\n",
      " 117 \n",
      "      Survived  Pclass     Sex   Age  SibSp  Parch     Fare Embarked\n",
      "47          1       3  female  28.0      0      0   7.7500        Q\n",
      "55          1       1    male  28.0      0      0  35.5000        S\n",
      "76          0       3    male  28.0      0      0   7.8958        S\n",
      "77          0       3    male  28.0      0      0   8.0500        S\n",
      "87          0       3    male  28.0      0      0   8.0500        S\n",
      "..        ...     ...     ...   ...    ...    ...      ...      ...\n",
      "870         0       3    male  26.0      0      0   7.8958        S\n",
      "877         0       3    male  19.0      0      0   7.8958        S\n",
      "878         0       3    male  28.0      0      0   7.8958        S\n",
      "884         0       3    male  25.0      0      0   7.0500        S\n",
      "886         0       2    male  27.0      0      0  13.0000        S\n",
      "\n",
      "[117 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# duplicados\n",
    "\n",
    "duplicates = titanic.duplicated()\n",
    "print(headr(\"Duplicadas originales:\"),duplicates.sum(),'\\n', titanic[duplicates])\n",
    "\n",
    "duplicates = titanic_cleaned.duplicated()\n",
    "print(headr(\"Duplicadas tras limpieza:\"),duplicates.sum(),'\\n', titanic_cleaned[duplicates])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Los duplicados son significativos y los dejaremos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volvemos a explorar con columnas codificadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "titanic_cleaned_coded = titanic_cleaned.copy()\n",
    "titanic_cleaned_coded[categorical_columns] = titanic_cleaned[categorical_columns].apply(LabelEncoder().fit_transform)   \n",
    "titanic_cleaned_coded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluamos la aportación de cada columna\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "X= titanic_cleaned_coded.drop(target_column, axis=1)\n",
    "y = titanic_cleaned_coded[target_column]\n",
    "\n",
    "fvalue_selector = SelectKBest(f_classif, k=2)\n",
    "\n",
    "X_kbest = fvalue_selector.fit(X,y)\n",
    "\n",
    "feature_scores = pd.DataFrame({\"Feature\": X.columns,\"Score\": X_kbest.scores_}).sort_values(by=\"Score\", ascending=False)\n",
    "\n",
    "print(headr(\"Feature scores\"))\n",
    "round(feature_scores,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver proporciones nuevamente\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "features = X.columns\n",
    "num_features = len(features)\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    print(headr(f\"Graficando: {feature}\"))\n",
    "    sns.barplot(x=feature, y=target_column, data=titanic_cleaned_coded)\n",
    "    plt.title(f\"Tasa de {target_column} por {feature}\")\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(target_column)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocesar Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminar constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[4mColumnas constantes:\u001b[0m\n",
      " Index(['Parch'], dtype='object')\n",
      "\n",
      "\u001b[4mColumnas finales:\u001b[0m\n",
      " Index(['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Fare', 'Embarked'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# titanic_cleaned=titanic_cleaned.drop('Parch', axis=1)\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold(threshold=0) # seleccionar características con varianza mayor a 0 o el valor que se desee\n",
    "sel.fit(titanic_cleaned_coded)\n",
    "\n",
    "no_constant_columns = sel.get_feature_names_out()\n",
    "constant_columns = titanic_cleaned.columns.drop(no_constant_columns)\n",
    "\n",
    "print(headr(\"Columnas constantes:\"), constant_columns)\n",
    "\n",
    "titanic_cleaned = titanic_cleaned.drop(constant_columns, axis=1)\n",
    "print(headr(\"Columnas finales:\"), titanic_cleaned.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparar para entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar características y target\n",
    "X = titanic_cleaned.drop(target_column, axis=1)\n",
    "y = titanic_cleaned[target_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir en entrenamiento y prueba\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Candidatos\n",
    "\n",
    "A partir del análisis previo:\n",
    "\n",
    "1. `LogisticRegression` # No coincide con perfil de datos, para comparar\n",
    "2. `RandomForestClassifier` \n",
    "3. `GradientBoostingClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.compose import ColumnTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Piplines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "\n",
    "numerical_columns = numerical_columns_selector(X)\n",
    "categorical_columns = categorical_columns_selector(X)\n",
    "\n",
    "print(headr(\"Numerical columns\"), numerical_columns)\n",
    "print(headr(\"Categorical columns\"), categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        ('numerical', StandardScaler(), numerical_columns),\n",
    "        ('categorical', OneHotEncoder(), categorical_columns)\n",
    "    ])\n",
    "\n",
    "pipelines = {\n",
    "    \"LogisticRegression\": Pipeline([('preprocessor', preprocessor),('classifier', LogisticRegression())]),\n",
    "    \"RandomForestClassifier\": Pipeline([('preprocessor', preprocessor),('classifier', RandomForestClassifier())]),\n",
    "    \"GradientBoostingClassifier\": Pipeline([('preprocessor', preprocessor),('classifier', GradientBoostingClassifier())]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_train(name, pipeline, cv):\n",
    "    print(headr(f\"Entrenar {name}\"))\n",
    "    cv_results = cross_validate(pipeline, X_train, y_train, cv=cv, scoring=\"accuracy\", return_estimator=True, return_train_score=True)\n",
    "    trained_model = cv_results[\"estimator\"][0]\n",
    "    scores = pd.DataFrame(cv_results)\n",
    "\n",
    "    print(\"test score (mean-std): {0:.2f} - {1:.2f}\".format(scores[\"test_score\"].mean(), scores[\"test_score\"].std()))\n",
    "    print(\"train score (mean-std): {0:.2f} - {1:.2f}\".format(scores[\"train_score\"].mean(), scores[\"train_score\"].std()))\n",
    "    print(\"params:\", pipeline.named_steps.get(\"classifier\").get_params())\n",
    "\n",
    "    y_pred = trained_model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    return {\"acc\": round(scores[\"test_score\"].mean(), 2), \"f1\": round(f1, 2),}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvss = ShuffleSplit(n_splits=40, test_size=0.2, random_state=0)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, pipeline in pipelines.items():\n",
    "    results[name] = cv_train(name, pipeline, cvss)\n",
    "\n",
    "print(headr(\"Resultados:\"))\n",
    "results_df=pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curva de aprenizaje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve, validation_curve\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curvas de aprendizaje\n",
    "\n",
    "train_sizes = np.linspace(0.1, 1.0, num=5, endpoint=True)\n",
    "\n",
    "def generate_learning_curves(name, pipeline, X, y, train_sizes):\n",
    "    results = learning_curve(pipeline, X, y, train_sizes=train_sizes,\n",
    "                             cv=cvss, scoring='accuracy')\n",
    "    \n",
    "    train_size, train_scores, test_scores = results[:3]\n",
    "\n",
    "    # graficar la curva.\n",
    "    plt.errorbar(train_size, train_scores.mean(axis=1),\n",
    "                 yerr=train_scores.std(axis=1), label=\"Error de entrenamiento\")\n",
    "    plt.errorbar(train_size, test_scores.mean(axis=1),\n",
    "                 yerr=test_scores.std(axis=1), label=\"Error de prueba\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.xscale(\"log\")\n",
    "    plt.xlabel(\"Número de muestras en el conjunto de entrenamiento\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.title(\"Curva de aprendizaje para {name}\".format(name=name))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pipeline_name, pipeline_obj in pipelines.items():\n",
    "    generate_learning_curves(pipeline_name, pipeline_obj, X, y, train_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curva validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curvas de validación\n",
    "\n",
    "def generate_validation_curves(name, pipeline, X, y, param_name, param_range):\n",
    "    train_scores, test_scores = validation_curve(\n",
    "        pipeline, X, y, param_name=param_name, param_range=param_range,\n",
    "        cv=cvss, scoring=\"accuracy\")\n",
    "\n",
    "    # graficar la curva.\n",
    "    plt.plot(param_range, train_scores.mean(\n",
    "        axis=1), label=\"Error de entrenamiento\")\n",
    "    plt.plot(param_range, test_scores.mean(axis=1), label=\"Error de prueba\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.xlabel(\"Valor del ({param_name})\".format(\n",
    "        param_name=param_name))\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Curva de validación para {name}\".format(name=name))\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pname = 'LogisticRegression'\n",
    "Cs = [0,0.1, 1, 5, 10, 12]\n",
    "generate_validation_curves(pname, pipelines[pname], X, y, 'classifier__C', Cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pname = 'RandomForestClassifier'\n",
    "n_estimators_ops = [1,2,3,4,5]\n",
    "generate_validation_curves(pname, pipelines[pname], X, y, 'classifier__n_estimators', n_estimators_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pname = 'GradientBoostingClassifier'\n",
    "n_estimators_ops = [1,2,3,4,5,10]\n",
    "generate_validation_curves(pname, pipelines[pname], X, y, 'classifier__n_estimators', n_estimators_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Afinar hiperparámentros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pname = 'GradientBoostingClassifier'\n",
    "\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [1, 2, 3, 4, 5, 10],\n",
    "    'classifier__max_depth': [1, 2, 3, 4, 5],\n",
    "    'classifier__learning_rate': [0.1, 0.01, 0.001]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipelines[pname], param_grid, cv=cvss, scoring=\"accuracy\")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(headr(pname))\n",
    "print(\"Mejores hiperparámetros:\", grid_search.best_params_)\n",
    "print(f\"Mejor accuracy: {grid_search.best_score_:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Afinar hiperparámetros otros modelos*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### re-entrenar + re-evaluar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        ('numerical', StandardScaler(), numerical_columns),\n",
    "        ('categorical', OneHotEncoder(), categorical_columns)\n",
    "    ])\n",
    "\n",
    "pipelines = {\n",
    "    \"LogisticRegression\": Pipeline([('preprocessor', preprocessor),('classifier', LogisticRegression())]),\n",
    "    \"RandomForestClassifier\": Pipeline([('preprocessor', preprocessor),('classifier', RandomForestClassifier())]),\n",
    "    \"GradientBoostingClassifier\": Pipeline([('preprocessor', preprocessor),('classifier', GradientBoostingClassifier(learning_rate=0.1, max_depth=3, n_estimators=10))]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_final = {}\n",
    "\n",
    "for name, pipeline in pipelines.items():\n",
    "    results_final[name] = cv_train(name, pipeline, cvss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparar resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(headr(\"Resultados:\"))\n",
    "results_df = pd.DataFrame(results_final)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Resultado\n",
    "\n",
    "GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Mejorar el resultado\n",
    "\n",
    "- Añadiendo más características: por ejemplo, procesando \"Cabin\" para generar una categoría\n",
    "- Usando otro tipo de codificación: por ejemplo, objetivo/codificación de media (target encoding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
