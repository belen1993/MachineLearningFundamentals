{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff29e09a",
   "metadata": {},
   "source": [
    "# Regresión lineal para una relación no lineal características-target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99290f0d",
   "metadata": {},
   "source": [
    "Mostraremos que incluso si la parametrización de los modelos lineales no se adapta de manera nativa al problema en cuestión, todavía es posible hacer que los modelos lineales sean más expresivos mediante la ingeniería de características (adicionales).\n",
    "\n",
    "Un pipeline de ML que combina un paso de ingeniería de características no lineal seguido de un paso de regresión lineal puede considerarse un modelo de regresión no lineal en su conjunto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944e7da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "\n",
    "n_sample = 100\n",
    "data_max, data_min = 1.4, -1.4\n",
    "len_data = (data_max - data_min)\n",
    "# sort the data to make plotting easier later\n",
    "data = np.sort(rng.rand(n_sample) * len_data - len_data / 2)\n",
    "noise = rng.randn(n_sample) * .3\n",
    "target = data ** 3 - 0.5 * data ** 2 + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68603e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "full_data = pd.DataFrame({\"input_feature\": data, \"target\": target})\n",
    "import seaborn as sns\n",
    "\n",
    "_ = sns.scatterplot(data=full_data, x=\"input_feature\", y=\"target\",\n",
    "                    color=\"black\", alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f4918a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Destacaremos las limitaciones de ajustar un modelo de regresión lineal \n",
    "data = data.reshape((-1, 1))\n",
    "data.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008b7200",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(data, target)\n",
    "target_predicted = linear_regression.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd0ecf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(target, target_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ca17a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(data=full_data, x=\"input_feature\", y=\"target\",\n",
    "                     color=\"black\", alpha=0.5)\n",
    "ax.plot(data, target_predicted)\n",
    "_ = ax.set_title(f\"Error cuadrático medio = {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bb16c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aquí el coeficiente e intercepción aprendidos por LinearRegression define la mejor \"línea recta\" que se ajusta a los datos. \n",
    "# Podemos inspeccionar los coeficientes utilizando los atributos del modelo aprendidos de la siguiente manera:\n",
    "\n",
    "print(f\"peso: {linear_regression.coef_[0]:.2f}, \"\n",
    "      f\"intercepción: {linear_regression.intercept_:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9a72f0",
   "metadata": {},
   "source": [
    "Es importante tener en cuenta que el modelo aprendido no podrá manejar la relación no lineal entre los datos y el objetivo, ya que los modelos lineales asumen que la relación entre los datos y el objetivo es lineal.\n",
    "\n",
    "De hecho, hay 3 posibilidades para resolver este problema:\n",
    "1. Elegir un modelo que pueda lidiar de forma nativa con la no linealidad,\n",
    "2. Generar, mediante ingeniería, un conjunto más rico de características (al incluir conocimiento experto) que pueda ser utilizado directamente por un modelo lineal simple, o\n",
    "3. Usar un \"núcleo (kernel)\" para tener una función de decisión local en lugar de una función de decisión lineal global."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b8215e",
   "metadata": {},
   "source": [
    "### 1. Usar un regresor de árbol de decisión que puede manejar de forma nativa la no linealidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c5edc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree = DecisionTreeRegressor(max_depth=3).fit(data, target)\n",
    "target_predicted = tree.predict(data)\n",
    "mse = mean_squared_error(target, target_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4199a50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(data=full_data, x=\"input_feature\", y=\"target\",\n",
    "                     color=\"black\", alpha=0.5)\n",
    "ax.plot(data, target_predicted)\n",
    "_ = ax.set_title(f\"Error cuadrático medio = {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e4efa9",
   "metadata": {},
   "source": [
    "### 2. En lugar de tener un modelo que pueda lidiar de forma nativa con la no linealidad, también podríamos modificar nuestros datos:\n",
    "- Podemos crear nuevas características, derivadas de las características originales, utilizando conocimiento experto. \n",
    "    - En este ejemplo, sabemos que tenemos una relación cúbica y cuadrada entre los datos y el objetivo (a partir de cómo se han generado los datos).\n",
    "- Podemos crear dos nuevas características (datos ** 2 y datos ** 3)\n",
    "    - **Este tipo de transformación se llama expansión de características polinómicas:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6fff22",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7e4110",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_expanded = np.concatenate([data, data ** 2, data ** 3], axis=1)\n",
    "data_expanded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c746080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression.fit(data_expanded, target)\n",
    "target_predicted = linear_regression.predict(data_expanded)\n",
    "mse = mean_squared_error(target, target_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875f0d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(data=full_data, x=\"input_feature\", y=\"target\",\n",
    "                     color=\"black\", alpha=0.5)\n",
    "ax.plot(data, target_predicted)\n",
    "_ = ax.set_title(f\"Error cuadrático medio = {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782818db",
   "metadata": {},
   "source": [
    "Podemos ver que incluso con un modelo lineal, podemos superar la limitación de linealidad del modelo agregando los componentes no lineales en el diseño de características adicionales.- Aquí, creamos nuevas características sabiendo la forma en que se generó el target.\n",
    "\n",
    "> **En lugar de crear manualmente estas características polinomiales, se puede usar directamente `sklearn.preprocessing.PolynomialFeatures`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd71bb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para demostrar el uso de la clase PolynomialFeatures, usamos un pipeline donde primero transforma las características y luego se ajusta al modelo de regresión.\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "polynomial_regression = make_pipeline(\n",
    "    PolynomialFeatures(degree=3, include_bias=False),\n",
    "    LinearRegression(),\n",
    ")\n",
    "polynomial_regression.fit(data, target)\n",
    "target_predicted = polynomial_regression.predict(data)\n",
    "mse = mean_squared_error(target, target_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2b01a2",
   "metadata": {},
   "source": [
    "En la celda anterior tuvimos que configurar `include_bias = False`, de lo contrario, crearíamos una columna perfectamente correlacionada con el `intercept_` introducido por LinearRegression. \n",
    "\n",
    "Podemos verificar que este procedimiento sea equivalente a crear las características a mano hasta un error numérico, calculando el máximo de los valores absolutos de las diferencias entre las características generadas por ambos métodos y verificación de que está cerca de cero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ca61ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(polynomial_regression[0].fit_transform(data) - data_expanded).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4136da00",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(data=full_data, x=\"input_feature\", y=\"target\",\n",
    "                     color=\"black\", alpha=0.5)\n",
    "ax.plot(data, target_predicted)\n",
    "_ = ax.set_title(f\"Error cuadrático medio = {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aef50cd",
   "metadata": {},
   "source": [
    "### 3. La última posibilidad es hacer que un modelo lineal sea más expresivo es usar un **“kernel”**.\n",
    "En lugar de aprender un peso por característica, como hemos enfatizado anteriormente, se asignará un peso a cada muestra.\n",
    "- Sin embargo, no todas las muestras se utilizarán.\n",
    "- Esta es la base del algoritmo *máquinas de vectores de soporte* (SVMs).\n",
    "\n",
    "Desarrollemos algunas intuiciones en la potencia expresiva relativa a SVMs con núcleos (kernels) **lineales** y **no lineales** al instalarlas en el mismo conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f84320c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero, SVM con un kernel lineal:\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "svr = SVR(kernel=\"linear\")\n",
    "svr.fit(data, target)\n",
    "target_predicted = svr.predict(data)\n",
    "mse = mean_squared_error(target, target_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a86b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(data=full_data, x=\"input_feature\", y=\"target\",\n",
    "                     color=\"black\", alpha=0.5)\n",
    "ax.plot(data, target_predicted)\n",
    "_ = ax.set_title(f\"Error cuadrático medio = {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89894997",
   "metadata": {},
   "source": [
    "El estimador también se puede configurar para usar un **núcleo no lineal**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "399ecce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# El resultado es otro tipo de modelo de regresión no lineal con una expresividad similar a la pipeline de regresión polinómica anterior:\n",
    "\n",
    "svr = SVR(kernel=\"poly\", degree=3)\n",
    "svr.fit(data, target)\n",
    "target_predicted = svr.predict(data)\n",
    "mse = mean_squared_error(target, target_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14629ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(data=full_data, x=\"input_feature\", y=\"target\",\n",
    "                     color=\"black\", alpha=0.5)\n",
    "ax.plot(data, target_predicted)\n",
    "_ = ax.set_title(f\"Error cuadrático medio = {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b871c684",
   "metadata": {},
   "source": [
    "Los métodos de kernel, como SVM, son muy eficientes para datasets pequeños-medianos.\n",
    "\n",
    "Para datasets más grandes con *n_samples >> 10_000*, a menudo es computacionalmente más eficiente realizar una expansión de características explícitas usando `PolynomialFeatures` u otros transformadores no lineales de Scikit-Learn como `KBinsDiscretizer` o `Nystroem`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6a71b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "binned_regression = make_pipeline(\n",
    "    KBinsDiscretizer(n_bins=8), LinearRegression(),\n",
    ")\n",
    "binned_regression.fit(data, target)\n",
    "target_predicted = binned_regression.predict(data)\n",
    "mse = mean_squared_error(target, target_predicted)\n",
    "\n",
    "ax = sns.scatterplot(data=full_data, x=\"input_feature\", y=\"target\",\n",
    "                     color=\"black\", alpha=0.5)\n",
    "ax.plot(data, target_predicted)\n",
    "_ = ax.set_title(f\"Error cuadrático medio = {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e08fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_approximation import Nystroem\n",
    "\n",
    "nystroem_regression = make_pipeline(\n",
    "    Nystroem(n_components=5), LinearRegression(),\n",
    ")\n",
    "nystroem_regression.fit(data, target)\n",
    "target_predicted = nystroem_regression.predict(data)\n",
    "mse = mean_squared_error(target, target_predicted)\n",
    "\n",
    "ax = sns.scatterplot(data=full_data, x=\"input_feature\", y=\"target\",\n",
    "                     color=\"black\", alpha=0.5)\n",
    "ax.plot(data, target_predicted)\n",
    "_ = ax.set_title(f\"Error cuadrático medio = {mse:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
