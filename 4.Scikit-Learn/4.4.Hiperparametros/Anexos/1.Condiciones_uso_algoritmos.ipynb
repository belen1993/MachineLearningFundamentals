{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condiciones de Uso de Algoritmos de Machine Learning en Scikit-Learn\n",
    "\n",
    "### **Regresión**\n",
    "| Algoritmo | Cuándo Usarlo | Condiciones de Uso |\n",
    "|-----------|-------------|--------------------|\n",
    "| **Regresión Lineal** (`LinearRegression`) | Cuando la relación entre variables es lineal. | Supone independencia de las variables predictoras, sin multicolinealidad significativa. |\n",
    "| **Regresión Ridge** (`Ridge`) | Cuando hay multicolinealidad en los datos. | Agrega regularización L2 para evitar sobreajuste. |\n",
    "| **Regresión Lasso** (`Lasso`) | Cuando se busca selección de características. | Usa regularización L1 para reducir coeficientes a cero, eliminando características irrelevantes. |\n",
    "| **ElasticNet** (`ElasticNet`) | Cuando se necesita un balance entre Ridge y Lasso. | Mezcla L1 y L2, útil cuando hay muchas variables correlacionadas. |\n",
    "| **Support Vector Regression (SVR)** (`SVR`) | Para problemas de regresión con datos no lineales. | Sensible a la escala de los datos, requiere normalización. |\n",
    "| **Random Forest Regressor** (`RandomForestRegressor`) | Cuando se necesita manejar relaciones no lineales y reducir el sobreajuste. | Adecuado para conjuntos de datos grandes y variables con relaciones complejas. |\n",
    "| **Gradient Boosting Regressor** (`GradientBoostingRegressor`) | Para obtener predicciones de alta precisión en problemas de regresión. | Puede ser propenso al sobreajuste si no se ajusta correctamente. |\n",
    "\n",
    "### **Clasificación**\n",
    "| Algoritmo | Cuándo Usarlo | Condiciones de Uso |\n",
    "|-----------|-------------|--------------------|\n",
    "| **Regresión Logística** (`LogisticRegression`) | Para clasificación binaria o multiclase cuando los datos son linealmente separables. | Supone independencia de predictores, sensible a valores atípicos. |\n",
    "| **K-Nearest Neighbors (KNN)** (`KNeighborsClassifier`) | Cuando se necesita un modelo simple y sin supuestos fuertes. | No es eficiente en grandes volúmenes de datos, sensible a la escala de las variables. |\n",
    "| **Support Vector Machine (SVM)** (`SVC`) | Para clasificación en espacios con dimensiones altas. | Sensible a la escala de los datos, puede ser costoso en grandes volúmenes. |\n",
    "| **Árbol de Decisión** (`DecisionTreeClassifier`) | Cuando se necesita un modelo interpretable y fácil de visualizar. | Propenso al sobreajuste si no se poda correctamente. |\n",
    "| **Random Forest** (`RandomForestClassifier`) | Para problemas con muchas variables y cuando se necesita robustez. | Reduce el sobreajuste de los árboles de decisión individuales. |\n",
    "| **Gradient Boosting** (`GradientBoostingClassifier`) | Para mejorar el rendimiento en problemas complejos de clasificación. | Sensible a hiperparámetros, puede ser más lento que Random Forest. |\n",
    "| **Naive Bayes** (`GaussianNB`, `MultinomialNB`) | Para clasificación de texto y datos categóricos. | Supone independencia entre las características, no siempre realista. |\n",
    "\n",
    "### **Agrupamiento (Clustering)**\n",
    "| Algoritmo | Cuándo Usarlo | Condiciones de Uso |\n",
    "|-----------|-------------|--------------------|\n",
    "| **K-Means** (`KMeans`) | Cuando se conoce el número de grupos en los datos. | Sensible a valores atípicos, requiere elección de `k`. |\n",
    "| **DBSCAN** (`DBSCAN`) | Para datos con densidad variable y detección de outliers. | No requiere definir el número de clusters, pero depende de `eps` y `min_samples`. |\n",
    "| **Mean Shift** (`MeanShift`) | Cuando no se conoce el número de clusters de antemano. | Automáticamente detecta el número de clusters, pero es computacionalmente costoso. |\n",
    "\n",
    "### **Reducción de Dimensionalidad**\n",
    "| Algoritmo | Cuándo Usarlo | Condiciones de Uso |\n",
    "|-----------|-------------|--------------------|\n",
    "| **PCA** (`PCA`) | Para reducir la dimensionalidad preservando la varianza de los datos. | Supone que los componentes principales explican la variabilidad más relevante. |\n",
    "| **t-SNE** (`TSNE`) | Para visualizar datos en espacios de alta dimensión. | Requiere ajuste fino de hiperparámetros (`perplexity` y `learning_rate`). |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
