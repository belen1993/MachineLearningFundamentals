{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"0\"></a>\n",
    "# **Métodos de Feature Engineering**\n",
    "\n",
    "La **Feature Engineering** o **ingeniería de características o variables** es el corazón de cualquier modelo de aprendizaje automático. \n",
    "\n",
    "El éxito de cualquier modelo de aprendizaje automático depende de la aplicación de varias técnicas de ingeniería de características. \n",
    "\n",
    "Por lo tanto, analizaremos varias técnicas que nos ayudarán a extraer, preparar y diseñar correctamente las características de nuestro conjunto de datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"0.1\"></a>\n",
    "## Tabla de contenidos\n",
    "\n",
    "1. [Introducción a la ingeniería de características](#1)\n",
    "2. [Descripción general de las técnicas de ingeniería de características](#2)\n",
    "3. [Imputación de datos faltantes](#3)\n",
    "   - 3.1 [Análisis de casos completos](#3.1)\n",
    "   - 3.2 [Imputación de media/mediana/moda](#3.2)\n",
    "   - 3.3 [Imputación de muestra aleatoria](#3.3)\n",
    "   - 3.4 [Reemplazo por valor arbitrario](#3.4)\n",
    "   - 3.5 [Imputación de fin de distribución](#3.5)\n",
    "   - 3.6 [Indicador de valor faltante](#3.6)\n",
    "4. [Codificación categórica](#4)\n",
    "   - 4.1 [Codificación One-Hot (OHE)](#4.1)\n",
    "   - 4.2 [Codificación ordinal](#4.2)\n",
    "   - 4.3 [Codificación de recuento y frecuencia](#4.3)\n",
    "   - 4.4 [Codificación de objetivo/media](#4.4)\n",
    "   - 4.5 [Peso de la evidencia](#4.5)\n",
    "5. [Transformación de variables](#5)\n",
    "   - 5.1 [Transformación logarítmica](#5.1)\n",
    "   - 5.2 [Transformación recíproca](#5.2)\n",
    "   - 5.3 [Transformación de raíz cuadrada](#5.3)\n",
    "   - 5.4 [Transformación exponencial](#5.4)\n",
    "   - 5.5 [Transformación de Box-Cox](#5.5)\n",
    "6. [Discretización](#6)\n",
    "   - 6.1 [Discretización de igual ancho con función de corte de pandas](#6.1)\n",
    "   - 6.2 [Discretización de igual frecuencia con función qcut de pandas [función](#6.2)\n",
    "   - 6.3 [Discretización del conocimiento del dominio](#6.3)\n",
    "7. [Ingeniería de valores atípicos](#7)\n",
    "   - 7.1 [Eliminación de valores atípicos](#7.1)\n",
    "   - 7.2 [Tratamiento de valores atípicos como valores faltantes](#7.2)\n",
    "   - 7.3 [Discretización](#7.3)\n",
    "   - 7.4 [Codificación superior/inferior/cero](#7.4)\n",
    "8. [Ingeniería de datos y tiempo](#8)\n",
    "9. [Referencias](#9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Introducción** <a class=\"anchor\" id=\"1\"></a>\n",
    "\n",
    "[Tabla de contenido](#0.1)\n",
    "\n",
    "> **La ingeniería de características es el proceso de usar el conocimiento del dominio para extraer características de datos sin procesar mediante técnicas de minería de datos. Estas características se pueden utilizar para mejorar el rendimiento de los algoritmos de aprendizaje automático. La ingeniería de características se puede considerar como aprendizaje automático aplicado en sí mismo**\n",
    "    \n",
    "- [https://en.wikipedia.org/wiki/Feature_engineering](https://en.wikipedia.org/wiki/Feature_engineering)\n",
    "\n",
    "Otra definición importante es la siguiente:-\n",
    "\n",
    "> **Idear características es difícil, requiere mucho tiempo y conocimientos especializados. El \"aprendizaje automático aplicado\" es básicamente ingeniería de características.**\n",
    "\n",
    "- Andrew Ng, Aprendizaje automático e IA a través de simulaciones cerebrales\n",
    "\n",
    "Por lo tanto, la ingeniería de características es el proceso de crear características útiles en un modelo de aprendizaje automático. Podemos ver que el éxito de cualquier modelo de aprendizaje automático depende de la aplicación de varias técnicas de ingeniería de características."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Descripción general de las técnicas** <a class=\"anchor\" id=\"2\"></a>\n",
    "\n",
    "[Tabla de contenido](#0.1)\n",
    "\n",
    "**Ingeniería de características** es un término muy amplio que consta de diferentes técnicas para procesar datos. Estas técnicas nos ayudan a procesar nuestros datos en bruto y convertirlos en datos procesados ​​listos para ser introducidos en un algoritmo de aprendizaje automático. \n",
    "\n",
    "Estas técnicas incluyen completar valores faltantes, codificar variables categóricas, transformar variables, crear nuevas variables a partir de las existentes y otras.\n",
    "\n",
    "Las técnicas que analizaremos son:\n",
    "\n",
    "1. Imputación de datos faltantes\n",
    "2. Codificación categórica\n",
    "3. Transformación de variables\n",
    "4. Discretización\n",
    "6. Ingeniería de valores atípicos\n",
    "7. Ingeniería de fecha y hora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Imputación de datos faltantes** <a class=\"anchor\" id=\"3\"></a>\n",
    "\n",
    "[Tabla de contenido](#0.1)\n",
    "\n",
    "- Los datos faltantes, o valores faltantes, ocurren cuando no se almacenan datos o valores para una determinada observación dentro de una variable.\n",
    "\n",
    "- Los datos faltantes **son una ocurrencia común y pueden tener un efecto significativo** en las conclusiones que se pueden extraer de los datos. Los datos incompletos son un problema inevitable al tratar con la mayoría de las fuentes de datos.\n",
    "\n",
    "- La **imputación** es el acto de reemplazar los datos faltantes con estimaciones estadísticas de los valores faltantes. El objetivo de cualquier técnica de imputación es producir un conjunto de datos completo que se pueda utilizar para entrenar modelos de aprendizaje automático.\n",
    "\n",
    "- Existen múltiples técnicas para la imputación de datos faltantes. Son los siguientes:\n",
    "\n",
    "1. Análisis de caso completo\n",
    "\n",
    "2. Imputación de media/mediana/moda\n",
    "\n",
    "3. Imputación de muestra aleatoria\n",
    "\n",
    "4. Reemplazo por valor arbitrario\n",
    "\n",
    "5. Imputación de fin de distribución\n",
    "\n",
    "6. Indicador de valor faltante\n",
    "\n",
    "7. Imputación multivariable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Mecanismos de datos faltantes**\n",
    "\n",
    "- Existen 3 mecanismos que conducen a la falta de datos, 2 de ellos implican la falta de datos de forma aleatoria o casi aleatoria, y el tercero implica una pérdida sistemática de datos.\n",
    "\n",
    "- #### **Falta completamente al azar, MCAR**\n",
    "\n",
    "    - Una variable es faltante completamente al azar (MCAR) si la probabilidad de que falte es la misma para todas las observaciones. Cuando los datos son MCAR, no existe absolutamente ninguna relación entre los datos faltantes y cualquier otro valor, observado o faltante, dentro del conjunto de datos. En otras palabras, esos puntos de datos faltantes son un subconjunto aleatorio de los datos. No ocurre nada sistemático que haga que algunos datos tengan más probabilidades de faltar que otros.\n",
    "\n",
    "    - Si los valores de las observaciones faltan completamente al azar, ignorar esos casos no sesgaría las inferencias realizadas.\n",
    "\n",
    "- #### **Falta al azar, MAR**\n",
    "\n",
    "    - MAR ocurre cuando existe una relación sistemática entre la propensión de valores faltantes y los datos observados. En otras palabras, la probabilidad de que falte una observación depende únicamente de la información disponible (otras variables en el conjunto de datos). Por ejemplo, si los hombres tienen más probabilidades de revelar su peso que las mujeres, el peso es MAR. La información sobre el peso faltará de forma aleatoria para aquellos hombres y mujeres que decidieron no revelar su peso, pero como los hombres son más propensos a revelarlo, habrá más valores faltantes para las mujeres que para los hombres.\n",
    "\n",
    "    - En una situación como la anterior, si decidimos continuar con la variable con valores faltantes (en este caso el peso), podríamos beneficiarnos de incluir el género para controlar el sesgo en el peso de las observaciones faltantes.\n",
    "\n",
    "- #### **Falta no aleatoria, MNAR**\n",
    "\n",
    "    - La falta de valores no es aleatoria (MNAR) si su falta depende de información no registrada en el conjunto de datos. En otras palabras, existe un mecanismo o una razón por la que se introducen valores faltantes en el conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.1 Análisis de caso completo (CCA)** <a class=\"anchor\" id=\"3.1\"></a>\n",
    "\n",
    "[Tabla de contenido](#0.1)\n",
    "\n",
    "- El **análisis de caso completo** implica analizar solo aquellas observaciones en el conjunto de datos que contienen valores en todas las variables. En otras palabras, en el análisis de caso completo **eliminamos todas las observaciones con valores faltantes**. Este procedimiento es adecuado cuando hay pocas observaciones con datos faltantes en el conjunto de datos.\n",
    "\n",
    "- **Por lo tanto, el análisis de caso completo (CCA)**, también llamado eliminación de casos por lista, consiste simplemente en descartar las observaciones en las que faltan valores en alguna de las variables. El análisis de caso completo significa literalmente analizar solo aquellas observaciones para las que hay información en todas las variables (X).\n",
    "\n",
    "- Pero, si el conjunto de datos contiene datos faltantes en múltiples variables, o algunas variables contienen una alta proporción de observaciones faltantes, podemos eliminar fácilmente una gran parte del conjunto de datos, y esto no es deseable.\n",
    "\n",
    "- El CCA se puede aplicar tanto a variables categóricas como numéricas.\n",
    "\n",
    "> En la práctica, el CCA puede ser un método aceptable cuando la cantidad de información faltante es pequeña. En muchos conjuntos de datos de la vida real, la cantidad de datos faltantes nunca es pequeña y, por lo tanto, el CCA normalmente nunca es una opción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CCA en dataset del Titanic**\n",
    "\n",
    "- Ahora, demostraremos la aplicación de CCA en el conjunto de datos del Titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # álgebra lineal\n",
    "import pandas as pd # procesamiento de datos, E/S de archivos CSV (p. ej. pd.read_csv)\n",
    "import matplotlib.pyplot as plt # para visualización de datos\n",
    "import seaborn as sns # para visualización de datos estadísticos\n",
    "import pylab\n",
    "import scipy.stats as stats\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "titanic = pd.read_csv('../../data/titanic/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# make a copy of titanic dataset\n",
    "data1 = titanic.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    " # check the percentage of missing values per variable\n",
    "\n",
    "data1.isnull().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ahora, si decidiéramos eliminar todas las observaciones faltantes, terminaríamos con un conjunto de datos muy pequeño, dado que Cabin falta en el 77 % de las observaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# comprobar cuántas observaciones descartaríamos\n",
    "print('total de pasajeros con valores en todas las variables: ', data1.dropna().shape[0])\n",
    "print('total de pasajeros en el Titanic: ', data1.shape[0])\n",
    "print('porcentaje de datos sin valores faltantes: ', data1.dropna().shape[0]/ (data1.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Por lo tanto, solo tenemos información completa para el 20 % de nuestras observaciones en el conjunto de datos del Titanic. \n",
    "    - **Por lo tanto, el CCA no sería una opción para este conjunto de datos.**\n",
    "\n",
    "> En conjuntos de datos con muchas variables que contienen datos faltantes, el CCA normalmente no será una opción, ya que producirá un conjunto de datos reducido con observaciones completas. Sin embargo, si solo se utilizará un subconjunto de las variables del conjunto de datos, podríamos evaluar variable por variable, ya sea que elijamos descartar valores con NA o reemplazarlos con otros métodos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "trusted": true
   },
   "source": [
    "## **3.2 Imputación de media/mediana/moda** <a class=\"anchor\" id=\"3.2\"></a>\n",
    "\n",
    "[Tabla de contenidos](#0.1)\n",
    "\n",
    "- Podemos reemplazar los valores faltantes con la media, la mediana o la moda de la variable. La imputación de media/mediana/moda se adopta ampliamente en organizaciones. \n",
    "- Aunque en la práctica esta técnica se utiliza en casi todas las situaciones, el procedimiento es adecuado **si los datos faltan de forma aleatoria y en pequeñas proporciones**. \n",
    "    - Si hay muchas observaciones faltantes, distorsionaremos la distribución de la variable, así como su relación con otras variables en el conjunto de datos. La distorsión en la distribución de la variable puede afectar el rendimiento de los modelos lineales.\n",
    "\n",
    "- La imputación de media/mediana consiste en reemplazar todas las ocurrencias de valores faltantes (NA) dentro de una variable por la media (si la variable tiene una distribución gaussiana) o la mediana (si la variable tiene una distribución sesgada).\n",
    "\n",
    "    - Para las variables **categóricas**, el reemplazo por la **moda** también se conoce como reemplazo por la categoría más frecuente.\n",
    "\n",
    "- Esta imputación asume que los datos faltan de forma completamente aleatoria (MCAR). \n",
    "    - Si este es el caso, podemos pensar en reemplazar el NA por la ocurrencia más frecuente de la variable, que es la **media si la variable tiene una distribución gaussiana**, o la **mediana en caso contrario**.\n",
    "    - La lógica es reemplazar la población de valores faltantes por el valor más frecuente, ya que esta es la ocurrencia más probable.\n",
    "\n",
    "- Al reemplazar el NA por la media o la mediana, **la varianza de la variable se distorsionará si el número de NA es grande** con respecto al número total de observaciones (ya que los valores imputados no difieren de la media ni entre sí). \n",
    "    - Por lo tanto, se produce una **subestimación de la varianza**.\n",
    "\n",
    "- Además, las estimaciones de **covarianza y correlaciones con otras variables en el conjunto de datos también pueden verse afectadas**. \n",
    "    - Esto se debe a que podríamos estar destruyendo correlaciones intrínsecas ya que la media/mediana que ahora reemplaza a NA no preservará la relación con las variables restantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Imputación en dataset del Titanic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# hacemos una copia del conjunto de datos del titanic\n",
    "data2 = titanic.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# comprobar el porcentaje de valores NA en el conjunto de datos\n",
    "data2.isnull().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Nota importante**\n",
    "\n",
    "- La imputación debe realizarse sobre el conjunto de entrenamiento y luego propagarse al conjunto de prueba. \n",
    "    - Esto significa que la media/mediana que se utilizará para completar los valores faltantes tanto en el conjunto de entrenamiento como en el de prueba debe extraerse únicamente del conjunto de entrenamiento. \n",
    "    - Esto es para evitar el sobreajuste.\n",
    "\n",
    "- En el conjunto de datos del Titanic, podemos ver que `Age` contiene 19,8653 %, `Cabin` contiene 77,10 % y `Embarked` contiene 0,22 % de valores faltantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Imputación de la variable Age**\n",
    "\n",
    "- `Edad` es una variable continua. Primero, comprobaremos la distribución de la variable `Age`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Graficamos la distribución de la edad para averiguar si es gaussiana o asimétrica.\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "fig = data2.Age.hist(bins=10)\n",
    "fig.set_ylabel('Número de pasajeros')\n",
    "fig.set_xlabel('Edad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Podemos observar que la distribución de la edad está sesgada, por lo que utilizaremos la imputación de la mediana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Separar el conjunto de datos en conjuntos de entrenamiento y de prueba\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data2, data2.Survived, test_size=0.3,\n",
    "random_state=0)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Calcular la mediana de la edad\n",
    "median = X_train.Age.median()\n",
    "median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# imputar valores faltantes en la edad en los conjuntos de prueba y entrenamiento\n",
    "\n",
    "for df in [X_train, X_test]:\n",
    "    df['Age'].fillna(median, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Verificamos si faltan valores en la variable `Age`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train['Age'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_test['Age'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Podemos ver que no hay valores faltantes en la variable `Age` en el conjunto de prueba y de tren.\n",
    "\n",
    "- Podemos seguir la misma línea y completar los valores faltantes en `Cabin` y `Embarked` con el valor más frecuente.\n",
    "\n",
    "- La **imputación de media/mediana/moda** es el método más común para imputar valores faltantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.3 Imputación de muestra aleatoria** <a class=\"anchor\" id=\"3.3\"></a>\n",
    "\n",
    "[Tabla de contenido](#0.1)\n",
    "\n",
    "- La imputación de muestra aleatoria se refiere a la **selección aleatoria de valores de la variable para reemplazar los datos faltantes**. \n",
    "- Esta técnica **preserva la distribución** de la variable y es adecuada para los datos faltantes al azar. Pero, necesitamos tener en cuenta la aleatoriedad estableciendo adecuadamente una semilla. De lo contrario, la misma observación faltante podría ser reemplazada por valores diferentes en diferentes ejecuciones de código y, por lo tanto, conducir a predicciones de modelos diferentes. Esto no es deseable cuando se utilizan nuestros modelos dentro de una organización.\n",
    "\n",
    "- El reemplazo de NA por muestreo aleatorio para variables categóricas es exactamente lo mismo que para las variables numéricas.\n",
    "\n",
    "- El muestreo aleatorio consiste en tomar una observación aleatoria del conjunto de observaciones disponibles de la variable, es decir, del conjunto de categorías disponibles, y usar ese valor extraído aleatoriamente para completar el NA. En el muestreo aleatorio, se toman tantas observaciones aleatorias como valores faltantes haya en la variable.\n",
    "\n",
    "- Mediante el muestreo aleatorio de las observaciones de las categorías actuales, garantizamos que se preserve la frecuencia de las diferentes categorías/etiquetas dentro de la variable.\n",
    "\n",
    "### Supuestos\n",
    "\n",
    "> La imputación de muestreo aleatorio presupone que los **datos faltan de forma completamente aleatoria (MCAR)**. Si este es el caso, tiene sentido sustituir los valores faltantes por valores extraídos de la distribución de la variable/frecuencia de la categoría original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Imputación en dataset del Titanic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# hacemos una copia del conjunto de datos del titanic\n",
    "data3 = titanic.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# % valores NA\n",
    "\n",
    "data3.isnull().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Nota importante**\n",
    "\n",
    "- La imputación debe realizarse sobre el conjunto de entrenamiento y luego propagarse al conjunto de prueba. \n",
    "- Esto significa que la muestra aleatoria que se utilizará para completar los valores faltantes tanto en el conjunto de entrenamiento como en el de prueba debe extraerse del conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Separamos el conjunto de datos en conjuntos de entrenamiento y prueba\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data3, data3.Survived, test_size=0.3,\n",
    "                                                    random_state=0)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# escribe una función para crear 3 variables a partir de Age:\n",
    "\n",
    "def impute_na(df, variable, median):\n",
    "    \n",
    "    df[variable+'_median'] = df[variable].fillna(median)\n",
    "    df[variable+'_zero'] = df[variable].fillna(0)\n",
    "    \n",
    "    # muestreo aleatorio\n",
    "    df[variable+'_random'] = df[variable]\n",
    "    \n",
    "    # extrae la muestra aleatoria para completar el na\n",
    "    random_sample = X_train[variable].dropna().sample(df[variable].isnull().sum(), random_state=0)\n",
    "    \n",
    "    # pandas necesita tener el mismo índice para fusionar conjuntos de datos\n",
    "    random_sample.index = df[df[variable].isnull()].index\n",
    "    df.loc[df[variable].isnull(), variable+'_random'] = random_sample\n",
    "    \n",
    "    # completa con random-sample\n",
    "    df[variable+'_random_sample'] = df[variable].fillna(random_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "impute_na(X_train, 'Age', median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "impute_na(X_test, 'Age', median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.4 Reemplazo por valor arbitrario** <a class=\"anchor\" id=\"3.4\"></a>\n",
    "\n",
    "[Tabla de contenido](#0.1)\n",
    "\n",
    "- El reemplazo por un valor arbitrario, como su nombre indica, se refiere a **reemplazar los datos faltantes por cualquier valor determinado arbitrariamente, pero el mismo valor para todos** los datos faltantes. \n",
    "- Este reemplazo es adecuado **si los datos no faltan al azar o si hay una gran proporción de valores faltantes**. \n",
    "    - Si todos los valores son positivos, un reemplazo típico es -1. \n",
    "    - Alternativamente, el reemplazo por 999 o -999 es una práctica común. \n",
    "    - Debemos** anticipar que estos valores arbitrarios no son una ocurrencia común** en la variable. \n",
    "\n",
    "- El reemplazo por valores arbitrarios puede no ser adecuado para modelos lineales, ya que es muy probable que distorsione la distribución de las variables y, por lo tanto, es posible que no se cumplan los supuestos del modelo.\n",
    "\n",
    "- Para las variables **categóricas**, esto es el equivalente a reemplazar las observaciones faltantes con la etiqueta **\"Faltantes\"**, que es un procedimiento ampliamente adoptado.\n",
    "\n",
    "- Se debe reemplazar el NA por valores arbitrarios cuando existen razones para creer que los NA no faltan al azar. \n",
    "    - En situaciones como esta, no nos gustaría reemplazarlos con la mediana o la media y, por lo tanto, hacer que el NA parezca la mayoría de nuestras observaciones.\n",
    "    - Queremos marcarlos. Queremos capturar la falta de algún modo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Imputación en dataset del Titanic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# copia\n",
    "\n",
    "data4 = titanic.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Vamos a separarlos en conjuntos de entrenamiento y de prueba.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data4, data4.Survived, test_size=0.3,\n",
    "                                                    random_state=0)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def impute_na(df, variable):\n",
    "    df[variable+'_zero'] = df[variable].fillna(0)\n",
    "    df[variable+'_hundred']= df[variable].fillna(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# reemplaza NA con el valor mediano en el conjunto de entrenamiento y prueba\n",
    "impute_na(X_train, 'Age')\n",
    "impute_na(X_test, 'Age')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "trusted": true
   },
   "source": [
    "- El valor arbitrario debe determinarse para cada variable en particular. \n",
    "    - Por ejemplo, para este conjunto de datos, la opción de reemplazar NA en edad por 0 o 100 es válida, porque ninguno de esos valores es frecuente en la distribución original de la variable y se encuentran en las colas de la distribución.\n",
    "\n",
    "- Sin embargo, si reemplazáramos NA en tarifa, esos valores ya no son buenos, porque podemos ver que la tarifa puede tomar valores de hasta 500. \n",
    "    - Por lo tanto, podríamos considerar usar 500 o 1000 para reemplazar NA en lugar de 100.\n",
    "\n",
    "- Podemos ver que esto es totalmente arbitrario. Pero se usa en la industria. Los valores típicos elegidos por las empresas son -9999 o 9999, o similares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.5 Imputación Final de la distribución** <a class=\"anchor\" id=\"3.5\"></a>\n",
    "\n",
    "[Tabla de contenido](#0.1)\n",
    "\n",
    "- Implica reemplazar los **valores faltantes por un valor en el extremo **más alejado de la cola de la distribución de la variable. \n",
    "- Esta técnica es similar en esencia a la imputación por un valor arbitrario. Sin embargo, al colocar el valor al final de la distribución, no necesitamos observar cada distribución de la variable individualmente, ya que el algoritmo lo hace automáticamente por nosotros. \n",
    "- Esta técnica de imputación tiende a **funcionar bien con algoritmos basados ​​en árboles**, pero puede afectar el rendimiento de los modelos lineales, ya que distorsiona la distribución de la variable.\n",
    "\n",
    "- En ocasiones, uno tiene razones para sospechar que los valores faltantes no faltan al azar. Y si el valor falta, tiene que haber una razón para ello. \n",
    "    - Por lo tanto, nos gustaría capturar esta información.\n",
    "\n",
    "- **Agregar una variable adicional** que indique la falta puede ayudar con esta tarea. Sin embargo, los valores siguen faltando en la variable original y deben reemplazarse si planeamos usar la variable en el aprendizaje automático.\n",
    "\n",
    "- Entonces, reemplazaremos el NA por valores que se encuentren en el extremo más alejado de la distribución de la variable.\n",
    "\n",
    "- La razón es que si falta el valor, debe ser por alguna concreta, por lo tanto, no nos gustaría reemplazar los valores faltantes por la media y hacer que esa observación se parezca a la mayoría de nuestras observaciones. \n",
    "    - En cambio, queremos marcar esa observación como diferente y, por lo tanto, asignamos un valor que se encuentra en la cola de la distribución, donde las observaciones rara vez están representadas en la población."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Imputación en dataset del Titanic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# copia\n",
    "\n",
    "data5 = titanic.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Separamos en conjuntos de entrenamiento y de prueba.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data5, data5.Survived, test_size=0.3,\n",
    "                                                    random_state=0)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "X_train.Age.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# en el extremo más alejado de la distribución\n",
    "X_train.Age.mean()+3*X_train.Age.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data5.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# podemos ver que hay algunos valores atípicos para Age\n",
    "# según su distribución, estos valores atípicos se ocultarán cuando reemplacemos NA por valores en el extremo más alejado\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(data5.Age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def impute_na(df, variable, median, extreme):\n",
    "    df[variable+'_far_end'] = df[variable].fillna(extreme)\n",
    "    df[variable].fillna(median, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# reemplacemos el NA con el valor mediano en los conjuntos de entrenamiento y prueba\n",
    "impute_na(X_train, 'Age', X_train.Age.median(), X_train.Age.mean()+3*X_train.Age.std())\n",
    "impute_na(X_test, 'Age', X_train.Age.median(), X_train.Age.mean()+3*X_train.Age.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.6 Indicador de valor faltante** <a class=\"anchor\" id=\"3.6\"></a>\n",
    "\n",
    "[Tabla de contenido](#0.1)\n",
    "\n",
    "- Implica **agregar una variable binaria para indicar si el valor falta** para una determinada observación. \n",
    "    - Esta variable toma el valor 1 si la observación falta o 0 en caso contrario. \n",
    "- Una cosa que se debe tener en cuenta es que aún necesitamos reemplazar los valores faltantes en la variable original, lo que solemos hacer con la imputación de media o mediana. \n",
    "- Al usar estas 2 técnicas juntas, si el valor faltante tiene poder predictivo, será capturado por el indicador faltante y, si no lo tiene, será enmascarado por la imputación de media/mediana.\n",
    "\n",
    "- Estas 2 técnicas en combinación **tienden a funcionar bien con modelos lineales**. \n",
    "    - Pero, agregar un indicador faltante expande el espacio de características y, como múltiples variables tienden a tener valores faltantes para las mismas observaciones, muchas de estas variables binarias recién creadas podrían ser idénticas o estar altamente correlacionadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Imputación en dataset del Titanic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# copia\n",
    "\n",
    "data6 = titanic.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Separamos en conjuntos de entrenamiento y de prueba.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data6, data6.Survived, test_size=0.3,\n",
    "                                                    random_state=0)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# creamos variable indicando falta\n",
    "\n",
    "X_train['Age_NA'] = np.where(X_train['Age'].isnull(), 1, 0)\n",
    "X_test['Age_NA'] = np.where(X_test['Age'].isnull(), 1, 0)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# podemos ver que la media y la mediana son similares, por lo que reemplazaremos con la mediana\n",
    "\n",
    "X_train.Age.mean(), X_train.Age.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# reemplazamos el NA con el valor mediano en el conjunto de entrenamiento\n",
    "X_train['Age'].fillna(X_train.Age.median(), inplace=True)\n",
    "X_test['Age'].fillna(X_train.Age.median(), inplace=True)\n",
    "\n",
    "X_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "trusted": true
   },
   "source": [
    "- Podemos ver que se crea otra variable `Age_NA` para capturar los datos faltantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Conclusión: cuándo utilizar cada método de imputación**\n",
    "\n",
    "- Si los valores faltantes son inferiores al 5 % de la variable, utilizamos la imputación de media/mediana o el reemplazo de muestra aleatoria. \n",
    "    - Imputar por categoría más frecuente si los valores faltantes son superiores al 5 % de la variable. \n",
    "    - Realizar la imputación de media/mediana + agregar una variable binaria adicional para capturar los valores faltantes y agregar una etiqueta \"Faltante\" en las variables categóricas.\n",
    "\n",
    "- Si la cantidad de valores faltantes en una variable es pequeña, es poco probable que tengan un fuerte impacto en la variable/objetivo que está tratando de predecir. \n",
    "    - Tratarlos de manera especial seguramente agregará ruido a las variables. \n",
    "    - Por lo tanto, es más útil reemplazar por media/muestra aleatoria para preservar la distribución de la variable.\n",
    "\n",
    "- Sin embargo, si la variable/objetivo que se está tratando de predecir está muy desequilibrada, entonces podría darse el caso de que esta pequeña cantidad de valores faltantes sea de hecho informativa.\n",
    "\n",
    "#### **Excepciones**\n",
    "\n",
    "- Si sospechamos que los NA no faltan al azar y no queremos atribuir la ocurrencia más común a NA, y si no queremos aumentar el espacio de características agregando una variable adicional para indicar la falta de datos, en estos casos, reemplace por un valor en el extremo más alejado de la distribución o un valor arbitrario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Codificación categórica** <a class=\"anchor\" id=\"4\"></a>\n",
    "\n",
    "[Tabla de contenido](#0.1)\n",
    "\n",
    "- Los datos categóricos son datos que toman una cantidad limitada de valores.\n",
    "\n",
    "    - Por ejemplo, si las personas respondieron a una encuesta sobre qué marca de automóvil poseen, el resultado sería categórico (porque las respuestas serían cosas como Honda, Toyota, Ford, Ninguno, etc.). Las respuestas se dividen en un conjunto fijo de categorías.\n",
    "\n",
    "- **Se obtendrá un error si se intenta conectar estas variables en la mayoría de los modelos de aprendizaje automático en Python sin \"codificarlas\" primero.**\n",
    "\n",
    "- La codificación de variables categóricas es un término amplio para las técnicas colectivas que se utilizan para transformar las cadenas o etiquetas de las variables categóricas en números. \n",
    "- Existen múltiples técnicas en este método:\n",
    "\n",
    "1. Codificación One-Hot (OHE)\n",
    "\n",
    "2. Codificación ordinal\n",
    "\n",
    "3. Codificación de recuento y frecuencia\n",
    "\n",
    "4. Codificación de objetivo/codificación de media\n",
    "\n",
    "5. Peso de la evidencia\n",
    "\n",
    "6. Codificación de etiquetas raras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.1 Codificación One-Hot (OHE)** <a class=\"anchor\" id=\"4.1\"></a>\n",
    "\n",
    "[Tabla de Contenidos](#0.1)\n",
    "\n",
    "- OHE es el enfoque estándar para codificar datos categóricos.\n",
    "\n",
    "- La codificación One-Hot (OHE) **crea una variable binaria para cada una de las diferentes categorías** presentes en una variable. \n",
    "    - Estas variables binarias toman 1 si la observación muestra una determinada categoría o 0 en caso contrario. \n",
    "    - **OHE es adecuada para modelos lineales.** \n",
    "    - OHE expande el espacio de características de manera bastante drástica si las variables categóricas son altamente cardinales, o si hay muchas variables categóricas. \n",
    "    - Además, muchas de las variables ficticias derivadas podrían estar altamente correlacionadas.\n",
    "    - Cada una de las variables booleanas también se conoce como variables **ficticias (dummies) o variables binarias**.\n",
    "\n",
    "- Por ejemplo, a partir de la variable categórica “Género”, con las etiquetas “femenino” y “masculino”, podemos generar la variable booleana “femenino”, que toma 1 si la persona es mujer o 0 en caso contrario. También podemos generar la variable masculino, que toma 1 si la persona es “masculino” y 0 en caso contrario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# copia\n",
    "\n",
    "data7 = titanic.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data7['Sex'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# OHE\n",
    "\n",
    "pd.get_dummies(data7['Sex']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# para una mejor visualización\n",
    "pd.concat([data7['Sex'], pd.get_dummies(data7['Sex'])], axis=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Podemos ver que solo necesitamos 1 de las 2 variables ficticias para representar la variable categórica original \"Sexo\". \n",
    "    - Cualquiera de las 2 hará el trabajo, y no importa cuál seleccionemos, ya que son equivalentes. \n",
    "    - Por lo tanto, para codificar una variable categórica con 2 etiquetas, solo necesitamos 1 variable ficticia.\n",
    "\n",
    "- De manera general, **para codificar una variable categórica con k etiquetas, necesitamos k-1 variables ficticias**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# obtención de etiquetas k-1\n",
    "pd.get_dummies(data7['Sex'], drop_first=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Veamos ahora un ejemplo con más de 2 etiquetas\n",
    "\n",
    "data7['Embarked'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# comprobar el número de etiquetas diferentes\n",
    "data7.Embarked.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# obtener el conjunto completo de variables ficticias\n",
    "\n",
    "pd.get_dummies(data7['Embarked']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# obtener k-1 variables ficticias\n",
    "\n",
    "pd.get_dummies(data7['Embarked'], drop_first=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La API de Scikt-Learn proporciona una clase para la codificación one-hot (https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html).\n",
    "\n",
    "- Además, una amplia gama de opciones de paquetes de codificación [Category Encoders](https://contrib.scikit-learn.org/category_encoders/).\n",
    "\n",
    "- Ambas opciones anteriores también se pueden usar para la codificación one-hot.\n",
    "\n",
    "## **Nota importante sobre OHE**\n",
    "\n",
    "- La [clase de codificador one hot](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder) de Scikit-learn solo acepta valores categóricos numéricos. \n",
    "    - Por lo tanto, cualquier valor de tipo cadena debe codificarse primero con la etiqueta antes de codificarse con one hot.\n",
    "\n",
    "- En el ejemplo del Titanic, el género de los pasajeros debe codificarse primero con la etiqueta antes de codificarse con one hot utilizando la clase de codificador one hot de Scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.2 Codificación ordinal** <a class=\"anchor\" id=\"4.2\"></a>\n",
    "\n",
    "[Tabla de contenido](#0.1)\n",
    "\n",
    "- Las variables categóricas cuyas categorías se pueden ordenar de forma significativa se denominan ordinales. Por ejemplo:\n",
    "\n",
    "    - La calificación del estudiante en un examen (A, B, C o reprobado).\n",
    "    - Los días de la semana pueden ser ordinales, con lunes = 1 y domingo = 7.\n",
    "    - Nivel educativo, con las categorías: escuela primaria, escuela secundaria, graduado universitario, doctorado, clasificadas del 1 al 4.\n",
    "\n",
    "- Cuando la variable categórica es ordinal, el enfoque más sencillo es reemplazar las etiquetas por algún número ordinal.\n",
    "\n",
    "- En la codificación ordinal, reemplazamos las categorías por dígitos, ya sea de manera arbitraria o de manera informada. \n",
    "    - Si codificamos las categorías de manera arbitraria, asignamos un número entero por categoría de 1 a n, donde n es el número de categorías únicas. \n",
    "    - Si, en cambio, asignamos los números enteros de manera informada, observamos la distribución objetivo: \n",
    "        - ordenamos las categorías de 1 a n, asignando 1 a la categoría para la cual las observaciones muestran la media más alta del valor objetivo y n a la categoría con el valor medio objetivo más bajo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Podemos utilizar el [Paquete de codificadores de categorías](https://contrib.scikit-learn.org/category_encoders/) para realizar la codificación ordinal. Consulta la documentación para obtener más información."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.3 Codificación de recuento y frecuencia** <a class=\"anchor\" id=\"4.3\"></a>\n",
    "\n",
    "[Tabla de contenido](#0.1)\n",
    "\n",
    "- En la codificación de recuento, **reemplazamos las categorías por el recuento de las observaciones** que muestran esa categoría en el conjunto de datos. \n",
    "- De manera similar, podemos reemplazar la categoría por la frecuencia (o porcentaje) de observaciones en el conjunto de datos. \n",
    "    - Es decir, si 10 de nuestras 100 observaciones muestran el color azul, reemplazaríamos el azul por 10 si hacemos una codificación de recuento, \n",
    "    - o por 0,1 si lo reemplazamos por la frecuencia. \n",
    "\n",
    "- Estas técnicas capturan la representación de cada etiqueta en un conjunto de datos, pero la codificación puede no ser necesariamente predictiva del resultado.\n",
    "\n",
    "- Veámoslo con un ejemplo diferente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#importamos dataset\n",
    "df_train = pd.read_csv('../../data/mercedes/train.zip')\n",
    "                       \n",
    "\n",
    "df_test = pd.read_csv('../../data/mercedes/test.zip') \n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Veamos cuántas etiquetas\n",
    "\n",
    "for col in df_train.columns[3:9]:\n",
    "    print(col, ': ', len(df_train[col].unique()), ' labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing count transformation of categorical variables, it is important to calculate the count (or frequency = count/total observations) over the training set, and then use those numbers to replace the labels in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_train[['X1', 'X2', 'X3', 'X4', 'X5', 'X6']], df_train.y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=0)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# obtengamos los recuentos de cada una de las etiquetas en la variable X2\n",
    "# capturemos esto en un diccionario que podamos usar para reasignar las etiquetas\n",
    "X_train.X2.value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# veamos X_train para que podamos comparar la recodificación de la variable\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# reemplazamos cada etiqueta en X2 por su conteo\n",
    "\n",
    "# primero hacemos un diccionario que asigna cada etiqueta a los conteos\n",
    "X_frequency_map = X_train.X2.value_counts().to_dict()\n",
    "\n",
    "# y ahora reemplazamos las etiquetas X2 tanto en el conjunto de entrenamiento como en el de prueba con el mismo mapa\n",
    "X_train.X2 = X_train.X2.map(X_frequency_map)\n",
    "X_test.X2 = X_test.X2.map(X_frequency_map)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donde en el conjunto de datos original, para la observación 1 en la variable 2 antes era 'ai', ahora fue reemplazado por el recuento 289. Y así sucesivamente para el resto de las categorías."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.4 Codificación de objetivo/media** <a class=\"anchor\" id=\"4.4\"></a>\n",
    "\n",
    "[Tabla de contenido](#0.1)\n",
    "\n",
    "- En la codificación de objetivo, también llamada codificación de media, **reemplazamos cada categoría de una variable por el valor medio del objetivo** para las observaciones que muestran una determinada categoría. \n",
    "    - Por ejemplo, tenemos la variable categórica “ciudad” y queremos predecir si el cliente comprará un televisor si le enviamos una carta. Si el 30 por ciento de las personas de la ciudad “Londres” compran el televisor, reemplazaríamos Londres por 0,3.\n",
    "\n",
    "- Esta técnica tiene 3 ventajas:\n",
    "\n",
    "1. no expande el espacio de características,\n",
    "\n",
    "2. captura cierta información sobre el objetivo en el momento de codificar la categoría, y\n",
    "\n",
    "3. crea una relación monótona entre la variable y el objetivo.\n",
    "\n",
    "- Las relaciones monótonas entre la variable y el objetivo tienden a mejorar el rendimiento del modelo lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Carguemos nuevamente el conjunto de datos del Titanic\n",
    "\n",
    "data = pd.read_csv('../../data/titanic/train.csv', usecols=['Cabin', 'Survived'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# completamos los valores NA con una etiqueta adicional\n",
    "\n",
    "data.Cabin.fillna('Missing', inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# comprobar el número de etiquetas diferentes en la cabina\n",
    "\n",
    "len(data.Cabin.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Ahora extraemos la primera letra de la cabina\n",
    "\n",
    "data['Cabin'] = data['Cabin'].astype(str).str[0]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# revisa las etiquetas\n",
    "data.Cabin.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importante**\n",
    "\n",
    "- El factor de riesgo debe calcularse por etiqueta considerando únicamente el conjunto de entrenamiento y luego ampliarse al conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Separamos en conjuntos de entrenamiento y de prueba.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[['Cabin', 'Survived']], data.Survived, test_size=0.3,\n",
    "                                                    random_state=0)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Calculamos la frecuencia objetivo para cada etiqueta\n",
    "\n",
    "X_train.groupby(['Cabin'])['Survived'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# y ahora hacemos lo mismo pero capturando el resultado en un diccionario\n",
    "\n",
    "ordered_labels = X_train.groupby(['Cabin'])['Survived'].mean().to_dict()\n",
    "ordered_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# reemplazamos las etiquetas con el 'riesgo' (frecuencia objetivo)\n",
    "# tener en cuenta que calculamos las frecuencias en función del conjunto de entrenamiento únicamente\n",
    "\n",
    "X_train['Cabin_ordered'] = X_train.Cabin.map(ordered_labels)\n",
    "X_test['Cabin_ordered'] = X_test.Cabin.map(ordered_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# resultados\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# gráfica de la variable original\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "fig = X_train.groupby(['Cabin'])['Survived'].mean().plot()\n",
    "fig.set_title('Normal relationship between variable and target')\n",
    "fig.set_ylabel('Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# gráfica del resultado transformado: la variable monótona\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "fig = X_train.groupby(['Cabin_ordered'])['Survived'].mean().plot()\n",
    "fig.set_title('Monotonic relationship between variable and target')\n",
    "fig.set_ylabel('Survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.5 Peso de la evidencia** <a class=\"anchor\" id=\"4.5\"></a>\n",
    "\n",
    "[Tabla de Contenidos](#0.1)\n",
    "\n",
    "- El peso de la evidencia (Weight of evidence: WOE) es una técnica utilizada para **codificar variables categóricas para su clasificación**. \n",
    "- WOE es el logaritmo natural de la probabilidad de que el objetivo sea 1 dividido por la probabilidad de que el objetivo sea 0. \n",
    "    - Su valor será 0 si el fenómeno es aleatorio.\n",
    "    - será mayor que 0 si la probabilidad de que el objetivo sea 0 es mayor.\n",
    "    - y será menor que 0 cuando la probabilidad de que el objetivo sea 1 es mayor.\n",
    "\n",
    "- La transformación WOE crea una bonita representación visual de la variable, porque al observar la variable codificada WOE, podemos ver, categoría por categoría, si favorece el resultado 0 o 1. \n",
    "- Además, WOE crea una relación monótona entre variable y objetivo, y deja todas las variables dentro del mismo rango de valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# previsualizar X_train\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ahora calculamos la probabilidad de objetivo=1\n",
    "X_train.groupby(['Cabin'])['Survived'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# dataframe con el cálculo anterior\n",
    "\n",
    "prob_df = X_train.groupby(['Cabin'])['Survived'].mean()\n",
    "prob_df = pd.DataFrame(prob_df)\n",
    "prob_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# y ahora la probabilidad del objetivo = 0\n",
    "# y la agregamos al dataframe\n",
    "\n",
    "prob_df = X_train.groupby(['Cabin'])['Survived'].mean()\n",
    "prob_df = pd.DataFrame(prob_df)\n",
    "prob_df['Died'] = 1-prob_df.Survived\n",
    "prob_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# dado que el logaritmo de cero no está definido, establezcamos este número en algo pequeño y distinto de cero\n",
    "\n",
    "prob_df.loc[prob_df.Survived == 0, 'Survived'] = 0.00001\n",
    "prob_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# calculamos el  WoE\n",
    "\n",
    "prob_df['WoE'] = np.log(prob_df.Survived/prob_df.Died)\n",
    "prob_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# y creamos un diccionario para reasignar la variable\n",
    "\n",
    "prob_df['WoE'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# y hacemos un diccionario para mapear la variable original al WoE\n",
    "# lo mismo que arriba pero capturamos el diccionario en una variable\n",
    "\n",
    "ordered_labels = prob_df['WoE'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# reemplazar las etiquetas con el WoE\n",
    "\n",
    "X_train['Cabin_ordered'] = X_train.Cabin.map(ordered_labels)\n",
    "X_test['Cabin_ordered'] = X_test.Cabin.map(ordered_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# verificar resultados\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Graficar la variable original\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "fig = X_train.groupby(['Cabin'])['Survived'].mean().plot()\n",
    "fig.set_title('Normal relationship between variable and target')\n",
    "fig.set_ylabel('Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# graficar el resultado transformado: la variable monótona\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "fig = X_train.groupby(['Cabin_ordered'])['Survived'].mean().plot()\n",
    "fig.set_title('Monotonic relationship between variable and target')\n",
    "fig.set_ylabel('Survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el gráfico anterior podemos observar que ahora existe una relación monótona entre la variable Cabin y la probabilidad de supervivencia. Cuanto mayor sea el número de Cabin, más probabilidades tenía la persona de sobrevivir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "trusted": true
   },
   "source": [
    "# **5. Transformación de variables** <a class=\"anchor\" id=\"5\"></a>\n",
    "\n",
    "[Tabla de contenido](#0.1)\n",
    "\n",
    "- Algunos modelos de aprendizaje automático, como la regresión lineal y logística, suponen que las variables se distribuyen normalmente. Otros se benefician de las distribuciones **similares a las gaussianas**, ya que en dichas distribuciones las observaciones de X disponibles para predecir Y varían en un rango mayor de valores. Por lo tanto, las variables distribuidas de manera gaussiana pueden mejorar el rendimiento del algoritmo de aprendizaje automático.\n",
    "\n",
    "- Si una variable no se distribuye normalmente, a veces es posible encontrar una transformación matemática para que la variable transformada sea gaussiana. Las transformaciones matemáticas que se utilizan habitualmente son:\n",
    "\n",
    "1. Transformación logarítmica: log(x)\n",
    "\n",
    "2. Transformación recíproca: 1/x\n",
    "\n",
    "3. Transformación de raíz cuadrada: sqrt(x)\n",
    "\n",
    "4. Transformación exponencial: exp(x)\n",
    "\n",
    "5. Transformación de Box-Cox\n",
    "\n",
    "- Ahora, demostremos las transformaciones anteriores en el conjunto de datos del Titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# cargar las variables numéricas del conjunto de datos del Titanic\n",
    "\n",
    "data = pd.read_csv('../../data/titanic/train.csv', usecols = ['Age', 'Fare', 'Survived'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Rellenamos los datos faltantes con una muestra aleatoria**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# primero rellenaré los datos faltantes de la variable edad, con una muestra aleatoria de la variableariable age, with a random sample of the variable\n",
    "\n",
    "def impute_na(data, variable):\n",
    "    # función para rellenar na con una muestra aleatoria\n",
    "    df = data.copy()\n",
    "    \n",
    "    # muestreo aleatorio\n",
    "    df[variable+'_random'] = df[variable]\n",
    "    \n",
    "    # extraigo la muestra aleatoria para rellenar na\n",
    "    random_sample = df[variable].dropna().sample(df[variable].isnull().sum(), random_state=0)\n",
    "    \n",
    "    # pandas necesita tener el mismo índice para poder fusionar conjuntos de datos\n",
    "    random_sample.index = df[df[variable].isnull()].index\n",
    "    df.loc[df[variable].isnull(), variable+'_random'] = random_sample\n",
    "    \n",
    "    return df[variable+'_random']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# imputar en nas\n",
    "data['Age'] = impute_na(data, 'Age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Age**\n",
    "\n",
    "### **Distribución original**\n",
    "\n",
    "- Podemos visualizar la distribución de la variable `Age`, trazando un histograma y el gráfico Q-Q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Graficar los histogramas para ver rápidamente las distribuciones\n",
    "# Podemos graficar gráficos Q-Q para visualizar si la variable se distribuye normalmente\n",
    "\n",
    "def diagnostic_plots(df, variable):\n",
    "    # Función para graficar un histograma y un gráfico Q-Q\n",
    "    # Uno al lado del otro, para una determinada variable\n",
    "    \n",
    "    plt.figure(figsize=(15,6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    df[variable].hist()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    stats.probplot(df[variable], dist=\"norm\", plot=pylab)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "diagnostic_plots(data, 'Age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La variable `Age` tiene una distribución casi normal, a excepción de algunas observaciones en la cola de valores inferiores de la distribución. Nótese la ligera desviación hacia la izquierda en el histograma y la desviación de la línea recta hacia los valores inferiores en el gráfico Q-Q.\n",
    "\n",
    "- En las siguientes celdas, aplicaré las transformaciones mencionadas anteriormente y compararé las distribuciones de la variable `Age` transformada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.1 Transformación logarítmica** <a class=\"anchor\" id=\"5.1\"></a>\n",
    "\n",
    "[Tabla de contenido](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data['Age_log'] = np.log(data.Age)\n",
    "\n",
    "diagnostic_plots(data, 'Age_log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La transformación logarítmica, no produjo una distribución tipo Gaussiana para la Edad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.2 Transformación recíproca** <a class=\"anchor\" id=\"5.2\"></a>\n",
    "\n",
    "[Tabla de contenidos](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data['Age_reciprocal'] = 1 / data.Age\n",
    "\n",
    "diagnostic_plots(data, 'Age_reciprocal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La transformación recíproca tampoco fue útil para transformar la Edad en una variable distribuida normalmente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.3 Transformación de raíz cuadrada** <a class=\"anchor\" id=\"5.3\"></a>\n",
    "\n",
    "[Tabla de contenido](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data['Age_sqr'] =data.Age**(1/2)\n",
    "\n",
    "diagnostic_plots(data, 'Age_sqr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La transformación de la raíz cuadrada es un poco más exitosa que las dos transformaciones anteriores. Sin embargo, la variable aún no es gaussiana y esto no representa una mejora hacia la normalidad respecto a la distribución original de Age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.4 Transformación exponencial** <a class=\"anchor\" id=\"5.4\"></a>\n",
    "\n",
    "[Tabla de contenido](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data['Age_exp'] = data.Age**(1/1.2) \n",
    "\n",
    "diagnostic_plots(data, 'Age_exp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La transformación exponencial es la mejor de todas las transformaciones anteriores, a la hora de generar una variable que se distribuye normalmente. Comparando el histograma y el gráfico Q-Q de la Edad transformada exponencialmente con la distribución original, podemos decir que la variable transformada sigue más de cerca una distribución gaussiana."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.5 Transformación de Box-Cox** <a class=\"anchor\" id=\"5.5\"></a>\n",
    "\n",
    "[Tabla de Contenidos](#0.1)\n",
    "\n",
    "- La transformación de Box-Cox se define como:\n",
    "\n",
    "     - T(Y)=(Y exp(λ)−1)/λ\n",
    "\n",
    "     - donde Y es la variable de respuesta y λ es el parámetro de transformación. λ varía de -5 a 5. En la transformación, se consideran todos los valores de λ y se selecciona el valor óptimo para una variable dada.\n",
    "\n",
    "- Brevemente, para cada λ (la transformación prueba varios λ), se calcula el coeficiente de correlación del gráfico de probabilidad (gráfico Q-Q a continuación, correlación entre valores ordenados y cuantiles teóricos).\n",
    "\n",
    "- El valor de λ correspondiente a la correlación máxima en el gráfico es entonces la opción óptima para λ.\n",
    "\n",
    "\n",
    "\n",
    "En Python, podemos evaluar y obtener el mejor λ con la función stats.boxcox del paquete scipy.\n",
    "\n",
    "- Podemos proceder de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data['Age_boxcox'], param = stats.boxcox(data.Age) \n",
    "\n",
    "print('Optimal λ: ', param)\n",
    "\n",
    "diagnostic_plots(data, 'Age_boxcox')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La transformación de Box Cox fue tan buena como la transformación exponencial que realizamos anteriormente para que Age pareciera más gaussiana. Si decidimos continuar con la variable original o la variable transformada, dependerá del propósito del ejercicio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "trusted": true
   },
   "source": [
    "# **6. Discretización** <a class=\"anchor\" id=\"6\"></a>\n",
    "\n",
    "[Tabla de contenido](#0.1)\n",
    "\n",
    "- La **discretización** es el proceso de transformar variables continuas en variables discretas mediante la creación de un conjunto de intervalos contiguos que abarcan el rango de valores de la variable.\n",
    "\n",
    "### La discretización ayuda a manejar valores atípicos y variables altamente sesgadas\n",
    "\n",
    "- La **discretización** ayuda a manejar valores atípicos al colocar estos valores en los intervalos inferiores o superiores junto con los valores internos restantes de la distribución. \n",
    "    - Por lo tanto, estas observaciones atípicas ya no difieren del resto de los valores en las colas de la distribución, ya que ahora están todos juntos en el mismo intervalo/cubo. Además, al crear contenedores o intervalos apropiados, la discretización puede ayudar a distribuir los valores de una variable sesgada en un conjunto de contenedores con la misma cantidad de observaciones.\n",
    "\n",
    "    - Existen varios enfoques para transformar variables continuas en discretas. \n",
    "    \n",
    "    - Este proceso también se conoce como **agrupamiento**, donde cada grupo representa cada intervalo.\n",
    "\n",
    "    - La **discretización** se refiere a la clasificación de los valores de la variable en grupos o intervalos, también llamados cubos/bins. \n",
    "\n",
    "- Existen múltiples formas de discretizar variables:\n",
    "\n",
    "1. Discretización de igual ancho\n",
    "\n",
    "2. Discretización de igual frecuencia\n",
    "\n",
    "3. Discretización de conocimiento del dominio\n",
    "\n",
    "4. Discretización mediante árboles de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Discretización de datos con las funciones cut y qcut de Pandas**\n",
    "\n",
    "- Cuando se trabaja con datos numéricos continuos, suele ser útil agrupar los datos en varios grupos para su posterior análisis. \n",
    "- Pandas admite estos enfoques mediante las funciones **cut** y **qcut**.\n",
    "\n",
    "    - El comando **cut** crea grupos equiespaciados, pero la frecuencia de las muestras es desigual en cada grupo.\n",
    "\n",
    "    - El comando **qcut** crea grupos de tamaño desigual, pero la frecuencia de las muestras es igual en cada grupo.\n",
    "\n",
    "- El siguiente diagrama ilustra este punto:\n",
    "\n",
    "![Discretising data with pandas cut and qcut](https://i.stack.imgur.com/pObHa.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6.1 Discretización de \"igual ancho\" con la función cut de Pandas** <a class=\"anchor\" id=\"6.1\"></a>\n",
    "\n",
    "[Tabla de contenido](#0.1)\n",
    "\n",
    "- La clasificación de igual ancho divide el alcance de los valores posibles en **N contenedores del mismo ancho**. \n",
    "    - El ancho está determinado por el rango de valores en la variable y la cantidad de contenedores que deseamos usar para dividir la variable.\n",
    "\n",
    "    - ancho = (valor máximo - valor mínimo) / N\n",
    "\n",
    "- Por ejemplo, si los valores de la variable varían entre 0 y 100, creamos 5 contenedores de esta manera: ancho = (100-0) / 5 = 20. \n",
    "    - Los contenedores son, por lo tanto, 0-20, 20-40, 40-60, 80-100. \n",
    "    - Los primeros y últimos contenedores (0-20 y 80-100) se pueden ampliar para dar cabida a valores atípicos (es decir, los valores inferiores a 0 o superiores a 100 también se colocarían en esos contenedores).\n",
    "\n",
    "- No existe una regla general para definir N. Normalmente, no querríamos más de 10.\n",
    "\n",
    "- Fuente: https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.cut.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# definir x\n",
    "x = np.array([24,  7,  2, 25, 22, 29])\n",
    "x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# discretización de ancho igual con corte\n",
    "pd.cut(x, bins = 3, labels = [\"bad\", \"medium\", \"good\"]).value_counts() #Bins size has equal interval of 9   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6.2 Discretización de frecuencias iguales con la función qcut de pandas** <a class=\"anchor\" id=\"6.2\"></a>\n",
    "\n",
    "[Tabla de contenido](#0.1)\n",
    "\n",
    "- La clasificación de frecuencias iguales divide el alcance de los valores posibles de la variable en N grupos, donde **cada grupo contiene la misma cantidad de observaciones**. \n",
    "- Esto es particularmente útil para las **variables sesgadas**, ya que distribuye las observaciones en los diferentes grupos de manera uniforme. Por lo general, encontramos los límites de intervalo determinando los cuantiles.\n",
    "\n",
    "- La discretización de frecuencias iguales **mediante cuantiles** consiste en dividir la variable continua en N cuantiles, N a definir por el usuario. No existe una regla general para definir N. Sin embargo, si pensamos en la variable discreta como una variable categórica, donde cada grupo es una categoría, nos gustaría mantener N (la cantidad de categorías) baja (normalmente no más de 10).\n",
    "\n",
    "- Fuente: https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.qcut.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# definir x\n",
    "x = np.array([24,  7,  2, 25, 22, 29])\n",
    "x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# discretización de frecuencias iguales con qcut\n",
    "pd.qcut(x, q = 3, labels = [\"bad\", \"medium\", \"good\"]).value_counts() #Equal frequency of 2 in each bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "trusted": true
   },
   "source": [
    "## **6.3 Discretización de conocimiento de dominio** <a class=\"anchor\" id=\"6.3\"></a>\n",
    "\n",
    "[Tabla de contenido](#0.1)\n",
    "\n",
    "- Con frecuencia, cuando se diseñan variables en un entorno empresarial, los expertos en negocios determinan los intervalos en los que creen que se debe dividir la variable para que tenga sentido para el negocio. \n",
    "    - Estos intervalos se pueden definir de manera arbitraria o siguiendo algún criterio de utilidad para el negocio. \n",
    "    - Ejemplos típicos son la discretización de variables como la edad y los ingresos.\n",
    "\n",
    "- Los ingresos, por ejemplo, suelen tener un límite máximo determinado y todos los ingresos por encima de ese valor se incluyen en el último grupo. \n",
    "- En cuanto a la edad, generalmente se divide en ciertos grupos según la necesidad del negocio; por ejemplo, la división en 0-21 (para menores de edad), 20-30 (para adultos jóvenes), 30-40, 40-60 y > 60 (para jubilados o próximos a jubilarse) es frecuente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# cargar las variables numéricas del dataset del Titanic\n",
    "data = pd.read_csv('../../data/titanic/train.csv', usecols = ['Age', 'Survived'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable Edad contiene datos faltantes, que completaré extrayendo una muestra aleatoria de la variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def impute_na(data, variable):\n",
    "    df = data.copy()\n",
    "    \n",
    "    # muestreo aleatorio\n",
    "    df[variable+'_random'] = df[variable]\n",
    "    \n",
    "    # extrae la muestra aleatoria para completar el na\n",
    "    random_sample = data[variable].dropna().sample(df[variable].isnull().sum(), random_state=0)\n",
    "    \n",
    "    # pandas necesita tener el mismo índice para fusionar conjuntos de datos\n",
    "    random_sample.index = df[df[variable].isnull()].index\n",
    "    df.loc[df[variable].isnull(), variable+'_random'] = random_sample\n",
    "    \n",
    "    return df[variable+'_random']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# imputar datos perdidos\n",
    "data['Age'] = impute_na(data, 'Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data['Age'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Dividamos la edad en categorías\n",
    "\n",
    "# Límites de categorías\n",
    "buckets = [0,20,40,60,100]\n",
    "\n",
    "# Etiquetas de categorías\n",
    "labels = ['0-20', '20-40', '40-60', '>60']\n",
    "\n",
    "# Discretización\n",
    "pd.cut(data.Age, bins = buckets, labels = labels, include_lowest=True).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# creamos dos nuevas columnas después de la discretización\n",
    "\n",
    "data['Age_buckets_labels'] = pd.cut(data.Age, bins=buckets, labels = labels, include_lowest=True)\n",
    "data['Age_buckets'] = pd.cut(data.Age, bins=buckets, include_lowest=True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Podemos observar los grupos en los que se colocó cada observación de edad. \n",
    "    - Por ejemplo, la edad de 27 años se colocó en el grupo de 20 a 40 años."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# número de pasajeros por grupo de edad\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "data.groupby('Age_buckets_labels')['Age'].count().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Podemos ver que hay diferentes pasajeros en cada etiqueta de grupo de edad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **7. Ingeniería de valores atípicos (outliers)** <a class=\"anchor\" id=\"7\"></a>\n",
    "\n",
    "[Tabla de contenido](#0.1)\n",
    "\n",
    "- Los valores atípicos son valores que son inusualmente altos o inusualmente bajos con respecto al resto de las observaciones de la variable. \n",
    "- Existen algunas técnicas para el manejo de valores atípicos:\n",
    "\n",
    "1. Eliminación de valores atípicos\n",
    "\n",
    "2. Tratamiento de valores atípicos como valores faltantes\n",
    "\n",
    "3. Discretización\n",
    "\n",
    "4. Codificación superior/inferior/cero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Identificación de valores atípicos**\n",
    "\n",
    "#### **Análisis de valores extremos**\n",
    "\n",
    "- La forma más básica de detección de valores atípicos es el análisis de valores extremos de datos unidimensionales. \n",
    "    - La clave de este método es determinar las colas estadísticas de la distribución subyacente de la variable y luego encontrar los valores que se encuentran en el extremo de las colas.\n",
    "\n",
    "- En el escenario típico, la distribución de la variable es gaussiana y, por lo tanto, los valores atípicos se ubicarán fuera de la media más o menos 3 veces la desviación estándar de la variable.\n",
    "\n",
    "- Si la variable no se distribuye normalmente, un enfoque general es calcular los cuantiles y luego el rango intercuantil (IQR), de la siguiente manera:\n",
    "\n",
    "    - IQR = cuantil 75 - cuantil 25\n",
    "\n",
    "- Un valor atípico se ubicará fuera de los siguientes límites superior e inferior:\n",
    "\n",
    "    - Límite superior = cuantil 75 + (IQR * 1,5)\n",
    "\n",
    "    - Límite inferior = cuantil 25 - (IQR * 1,5)\n",
    "\n",
    "- o para casos extremos:\n",
    "\n",
    "    - Límite superior = cuantil 75 + (IQR * 3)\n",
    "\n",
    "    - Límite inferior = cuantil 25 - (IQR * 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7.1 Eliminación de valores atípicos** <a class=\"anchor\" id=\"7.1\"></a>\n",
    "\n",
    "[Tabla de contenido](#0.1)\n",
    "\n",
    "- La eliminación de valores atípicos se refiere a la eliminación de observaciones atípicas del conjunto de datos. \n",
    "- Los valores atípicos, por naturaleza, no son abundantes, por lo que este procedimiento no debería distorsionar drásticamente el conjunto de datos. Pero si hay valores atípicos en múltiples variables, podemos terminar eliminando una gran parte del conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7.2 Tratamiento de valores atípicos como valores faltantes** <a class=\"anchor\" id=\"7.2\"></a>\n",
    "\n",
    "[Tabla de contenido](#0.1)\n",
    "\n",
    "- Podemos tratar los valores atípicos como información faltante y llevar a cabo cualquiera de los métodos de imputación descritos anteriormente en este núcleo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7.3 Discretización** <a class=\"anchor\" id=\"7.3\"></a>\n",
    "\n",
    "[Tabla de contenido](#0.1)\n",
    "\n",
    "- La discretización gestiona los valores atípicos automáticamente, ya que estos se clasifican en los contenedores terminales, junto con las otras observaciones de valores más altos o más bajos. Los mejores enfoques son la discretización basada en árboles y de frecuencias iguales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7.4 Codificación superior/inferior/cero** <a class=\"anchor\" id=\"7.4\"></a>\n",
    "\n",
    "[Tabla de contenido](#0.1)\n",
    "\n",
    "- La codificación superior o inferior también se conoce como **Winsorización** o **limitación de valores atípicos**. \n",
    "- El procedimiento implica limitar los valores máximos y mínimos a un valor predefinido. Este valor predefinido puede ser arbitrario o puede derivarse de la distribución de la variable.\n",
    "    - Si la variable se distribuye normalmente, podemos limitar los valores máximos y mínimos a la media más o menos 3 veces la desviación estándar. \n",
    "    - Si la variable está sesgada, podemos utilizar la regla de proximidad de rango intercuantil o limitar los percentiles superior e inferior.\n",
    "\n",
    "- Esto se demuestra utilizando el siguiente dataset del titanic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# cargar las variables numéricas  del Titanic\n",
    "data = pd.read_csv('../../data/titanic/train.csv', usecols = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Survived'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **La codificación superior es importante**\n",
    "\n",
    "La codificación superior y la codificación inferior, como cualquier otro paso de preprocesamiento de características, deben determinarse sobre el conjunto de entrenamiento y luego transferirse al conjunto de prueba. \n",
    "\n",
    "Esto significa que debemos encontrar los límites superior e inferior solo en el conjunto de entrenamiento y usar esas bandas para limitar los valores en el conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# dividimos el conjunto de datos en un conjunto de prueba y otro de entrenamiento\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, data.Survived,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=0)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Valores atípicos en variables continuas**\n",
    "\n",
    "- Podemos ver que `Age` y `Fare` son variables continuas. Por lo tanto, primero limitaremos los valores atípicos en esas variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Hagamos diagramas de caja para visualizar valores atípicos en las variables continuas\n",
    "# Edad y tarifa\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.subplot(1, 2, 1)\n",
    "fig = data.boxplot(column='Age')\n",
    "fig.set_title('')\n",
    "fig.set_ylabel('Age')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "fig = data.boxplot(column='Fare')\n",
    "fig.set_title('')\n",
    "fig.set_ylabel('Fare')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tanto Age como Fare contienen valores atípicos. Busquemos qué valores son atípicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Primero, graficamos las distribuciones para averiguar si son gaussianas o asimétricas.\n",
    "# Dependiendo de la distribución, utilizaremos el supuesto normal o el rango intercuantil\n",
    "# para encontrar valores atípicos\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.subplot(1, 2, 1)\n",
    "fig = data.Age.hist(bins=20)\n",
    "fig.set_ylabel('Number of passengers')\n",
    "fig.set_xlabel('Age')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "fig = data.Fare.hist(bins=20)\n",
    "fig.set_ylabel('Number of passengers')\n",
    "fig.set_xlabel('Fare')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La edad es bastante gaussiana y la tarifa está sesgada, por lo que utilizaré el supuesto gaussiano para la edad y el rango intercuartil para la tarifa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# encontrar valores atípicos\n",
    "\n",
    "# Age\n",
    "Upper_boundary = data.Age.mean() + 3* data.Age.std()\n",
    "Lower_boundary = data.Age.mean() - 3* data.Age.std()\n",
    "print('Age outliers are values < {lowerboundary} or > {upperboundary}'.format(lowerboundary=Lower_boundary, upperboundary=Upper_boundary))\n",
    "\n",
    "# Fare\n",
    "IQR = data.Fare.quantile(0.75) - data.Fare.quantile(0.25)\n",
    "Lower_fence = data.Fare.quantile(0.25) - (IQR * 3)\n",
    "Upper_fence = data.Fare.quantile(0.75) + (IQR * 3)\n",
    "print('Fare outliers are values < {lowerboundary} or > {upperboundary}'.format(lowerboundary=Lower_fence, upperboundary=Upper_fence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Age**\n",
    "\n",
    "- En el caso de la variable Edad, los valores atípicos se encuentran solo a la derecha de la distribución. Por lo tanto, solo necesitamos introducir la codificación superior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ver el resumen estadístico de Edad\n",
    "data.Age.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Suponiendo normalidad\n",
    "\n",
    "Upper_boundary = X_train.Age.mean() + 3* X_train.Age.std()\n",
    "Upper_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# codificación superior de la variable Age\n",
    "\n",
    "X_train.loc[X_train.Age>73, 'Age'] = 73\n",
    "X_test.loc[X_test.Age>73, 'Age'] = 73\n",
    "\n",
    "X_train.Age.max(), X_test.Age.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Fare**\n",
    "\n",
    "- Los valores atípicos, según el gráfico anterior, se encuentran todos en el lado derecho de la distribución. \n",
    "- Es decir, algunas personas pagaron precios extremadamente altos por sus entradas. \n",
    "- Por lo tanto, en esta variable, solo los valores extremadamente altos afectarán el rendimiento de nuestros modelos de aprendizaje automático y, por lo tanto, debemos realizar una codificación superior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ver propiedades estadísticas de Fare\n",
    "\n",
    "X_train.Fare.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# codificación superior: límite superior para valores atípicos según la regla de proximidad intercuantil\n",
    "\n",
    "IQR = data.Fare.quantile(0.75) - data.Fare.quantile(0.25)\n",
    "\n",
    "Upper_fence = X_train.Fare.quantile(0.75) + (IQR * 3)\n",
    "\n",
    "Upper_fence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El límite superior, por encima del cual cada valor se considera atípico, es un coste de 100 dólares por tarifa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# top-coding: capping the variable Fare at 100\n",
    "X_train.loc[X_train.Fare>100, 'Fare'] = 100\n",
    "X_test.loc[X_test.Fare>100, 'Fare'] = 100\n",
    "X_train.Fare.max(), X_test.Fare.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta manera abordamos los valores atípicos desde una perspectiva de ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **8. Ingeniería de fecha y hora** <a class=\"anchor\" id=\"8\"></a>\n",
    "\n",
    "[Tabla de contenido](#0.1)\n",
    "\n",
    "- Las variables de fecha son un tipo especial de variable categórica. \n",
    "    - Por su propia naturaleza, las variables de fecha contendrán una multitud de etiquetas diferentes, cada una correspondiente a una fecha y, a veces, a una hora específicas. \n",
    "    - Las variables de fecha, cuando se preprocesan correctamente, pueden enriquecer enormemente un conjunto de datos. \n",
    "- Por ejemplo, de una variable de fecha podemos extraer:\n",
    "    - Mes\n",
    "    - Trimestre\n",
    "    - Semestre\n",
    "    - Día (número)\n",
    "    - Día de la semana\n",
    "    - ¿Es fin de semana?\n",
    "    - Hora\n",
    "    - Diferencias horarias en años, meses, días, horas, etc.\n",
    "\n",
    "- Es importante entender que las variables de fecha **no deben usarse como las variables categóricas** con las que hemos estado trabajando hasta ahora al crear un modelo de aprendizaje automático. \n",
    "    - No solo porque tienen una multitud de categorías, sino también porque cuando realmente usamos el modelo para puntuar una nueva observación, es muy probable que esta observación sea en el futuro y, por lo tanto, su etiqueta de fecha será diferente a las contenidas en el conjunto de entrenamiento y, por lo tanto, a las utilizadas para entrenar el algoritmo de aprendizaje automático.\n",
    "\n",
    "- Usaremos el dataset del club de préstamos para los ejemplos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# carguamos el conjunto de datos de Lending Club con las columnas y filas seleccionadas\n",
    "\n",
    "use_cols = ['issue_d', 'last_pymnt_d']\n",
    "data = pd.read_csv('../../data/loan.csv', usecols=use_cols, nrows=50000)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Ahora analicemos las fechas, actualmente codificadas como cadenas, en formato de fecha y hora.\n",
    "\n",
    "data['issue_dt'] = pd.to_datetime(data.issue_d)\n",
    "data['last_pymnt_dt'] = pd.to_datetime(data.last_pymnt_d)\n",
    "\n",
    "data[['issue_d','issue_dt','last_pymnt_d', 'last_pymnt_dt']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Extraer mes de la fecha\n",
    "\n",
    "data['issue_dt_month'] = data['issue_dt'].dt.month\n",
    "\n",
    "data[['issue_dt', 'issue_dt_month']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data[['issue_dt', 'issue_dt_month']].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Extraer trimestre de la variable fecha\n",
    "\n",
    "data['issue_dt_quarter'] = data['issue_dt'].dt.quarter\n",
    "\n",
    "data[['issue_dt', 'issue_dt_quarter']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data[['issue_dt', 'issue_dt_quarter']].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# También podríamos extraer semestre\n",
    "\n",
    "data['issue_dt_semester'] = np.where(data.issue_dt_quarter.isin([1,2]),1,2)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# día - numérico del 1-31\n",
    "\n",
    "data['issue_dt_day'] = data['issue_dt'].dt.day\n",
    "\n",
    "data[['issue_dt', 'issue_dt_day']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# día de la semana - de 0 a 6\n",
    "\n",
    "data['issue_dt_dayofweek'] = data['issue_dt'].dt.dayofweek\n",
    "\n",
    "data[['issue_dt', 'issue_dt_dayofweek']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data[['issue_dt', 'issue_dt_dayofweek']].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# día de la semana - nombre\n",
    "\n",
    "data['issue_dt_dayofweek'] = data['issue_dt'].dt.day_name()\n",
    "\n",
    "data[['issue_dt', 'issue_dt_dayofweek']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data[['issue_dt', 'issue_dt_dayofweek']].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ¿El pedido se realizó el fin de semana?\n",
    "\n",
    "data['issue_dt_is_weekend'] = np.where(data['issue_dt_dayofweek'].isin(['Sunday', 'Saturday']), 1,0)\n",
    "data[['issue_dt', 'issue_dt_dayofweek','issue_dt_is_weekend']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data[data.issue_dt_is_weekend==1][['issue_dt', 'issue_dt_dayofweek','issue_dt_is_weekend']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# extraer año \n",
    "\n",
    "data['issue_dt_year'] = data['issue_dt'].dt.year\n",
    "\n",
    "data[['issue_dt', 'issue_dt_year']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#extrae la diferencia de fecha entre 2 fechas\n",
    "\n",
    "data['issue_dt'] - data['last_pymnt_dt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **9. Referencias** <a class=\"anchor\" id=\"9\"></a>\n",
    "\n",
    "[Tabla de contenido](#0.1)\n",
    "\n",
    "\n",
    "1. Python Feature Engineering Cookbook: Over 70 recipes for creating, engineering, and transforming features to build machine learning models. Soledad Galli 2022.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ir al inicio](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **10. Ejercicio** <a class=\"anchor\" id=\"9\"></a>\n",
    "\n",
    "[Tabla de contenido](#0.1)\n",
    "\n",
    "\n",
    "Aplica los métodos vistos al dataset de *Autos de 1985* (data/fe/autos.csv)\n",
    "\n",
    "- Imputación de valores faltantes.\n",
    "- Codificación categórica.\n",
    "- Transformación de variables (price).\n",
    "- Discretización (agrupar engine_size en categorías).\n",
    "- Eliminación de valores atípicos (price, horsepower).\n",
    "- Datos y tiempo (crear una nueva variable power_to_weight_ratio).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
