{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cheatsheet Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Métodos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputación\n",
    "\n",
    "✔️ Cuando hay datos faltantes\n",
    "\n",
    "- Si los valores faltantes son inferiores al 5 % de la variable: media/mediana o el reemplazo de muestra aleatoria. \n",
    "    - Si es normal o quasi-normal: media, si no mediana\n",
    "    - Imputar por categoría más frecuente si los valores faltantes son superiores al 5 % de la variable. \n",
    "    - Realizar la imputación de media/mediana + agregar una variable binaria adicional para capturar los valores faltantes y agregar una etiqueta \"Faltante\" en las variables categóricas.\n",
    "\n",
    "- Si la cantidad de valores faltantes en una variable es pequeña: media/muestra aleatoria, para preservar la distribución de la variable.\n",
    "- Si la variable/objetivo que se está tratando de predecir está muy desequilibrada: valores faltantes sea de hecho informativa.\n",
    "\n",
    "- Si MNAR y no queremos atribuir la ocurrencia más común a NA, y si no queremos variable adicional para indicar la falta de datos:reemplazar por un valor en el extremo más alejado de la distribución o un valor arbitrario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codificación\n",
    "\n",
    "✔️ Para transformar varibles categóricas en números"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Codificación One-Hot (OHE):** modelos lineales `pd.get_dummies(<column>, drop_first=True)`\n",
    "\n",
    "2. **Codificación ordinal:** reemplazar las etiquetas por algún número ordinal\n",
    "\n",
    "3. **Codificación de recuento y frecuencia:** Cuando queremos capturar la importancia de la frecuencia de categoría (causa problemas si diferentes categorías tienen frecuencias similares).\n",
    "\n",
    "4. **Codificación de objetivo/codificación de media:** introducir una característica fuerte en el modelo si la media objetivo es predictiva del resultado.\n",
    "\n",
    "5. **Peso de la evidencia:** para codificar variables categóricas para su clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métodos 3-5\n",
    "X_to_map = X_train[<columna>].value_counts().to_dict() # frecuencia\n",
    "\n",
    "X_to_map = X_train.groupby(['<col1>'])['target'].mean().to_dict() # media\n",
    "\n",
    "df['WoE'] = np.log(df['<target>']/df['<prob_of_columns>']) # WoE\n",
    "X_to_map = df['WoE'].to_dict()\n",
    "\n",
    "\n",
    "# reemplazamos las etiquetas\n",
    "X_train.X2 = X_train.X2.map(X_to_map)\n",
    "X_test.X2 = X_test.X2.map(X_to_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformación\n",
    "\n",
    "✔️ Para ayudar a hacer una característica más normal (normalizar).\n",
    "\n",
    "1. Transformación logarítmica: log(x)\n",
    "\n",
    "2. Transformación recíproca: 1/x\n",
    "\n",
    "3. Transformación de raíz cuadrada: sqrt(x)\n",
    "\n",
    "4. Transformación exponencial: exp(x)\n",
    "\n",
    "5. Transformación de Box-Cox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['<column_log>'] = np.log(df['<column>']) # tx log\n",
    "df['column_boxcox'], param = stats.boxcox(df['<column>']) # tx box-cox\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogramas + Q-Q para visualizar si la variable se distribuye normalmente\n",
    "\n",
    "def diagnostic_plots(df, variable):\n",
    "    # Función para graficar un histograma y un gráfico Q-Q\n",
    "    # Uno al lado del otro, para una determinada variable\n",
    "    \n",
    "    plt.figure(figsize=(15,6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    df[variable].hist()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    stats.probplot(df[variable], dist=\"norm\", plot=pylab)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "diagnostic_plots(df, '<column>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estandarizar: \n",
    "✔️ Cuando las escalas de las características son muy distintas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretización\n",
    "✔️ Para discretizR valores. Por ejemplo, para mejorar el performance de modelos lineales o eliminar Outliers.\n",
    "\n",
    "1. Discretización de igual ancho: `pd.cut(<column>, bins = <num>, labels = [<labels>])`\n",
    "2. Discretización de igual frecuencia: `qcutpd.qcut(<column>, q = <num>, labels = [<labes>])`\n",
    "\n",
    "3. Discretización de conocimiento del dominio\n",
    "\n",
    "4. Discretización mediante árboles de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valores atípicos\n",
    "\n",
    "✔️ Para identificar y eliminar valores atípicos\n",
    "\n",
    "1. **Eliminación de valores atípicos:** < 2%\n",
    "\n",
    "2. **Tratamiento de valores atípicos como valores faltantes:** < 5%\n",
    "\n",
    "3. **Discretización:** cuando lo requiera el modelo (modelos lineales).\n",
    "\n",
    "4. **Codificación superior/inferior/cero:** \n",
    "    - Normal: 3*std\n",
    "    - No Normal: IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encontrar valores atípicos\n",
    "\n",
    "# Normal\n",
    "upper_bound = data['<column>'].mean() + 3* data['<column>'].std()\n",
    "lower_bound = data['<column>'].mean() - 3* data['<column>'].std()\n",
    "\n",
    "# No notrmal\n",
    "IQR = data['<column>'].quantile(0.75) - data['<column>'].quantile(0.25)\n",
    "lower_bound = data['<column>'].quantile(0.25) - (IQR * 3)\n",
    "upper_bound = data['<column>'].quantile(0.75) + (IQR * 3)\n",
    "\n",
    "# Identifiación\n",
    "outliers = df[(df['<column>'] < lower_bound) | (df['<column>'] > upper_bound)]\n",
    "\n",
    "# Opción 1: Eliminar outliers\n",
    "outliers_removed = df[(df['<column>'] >= lower_bound) &  (df['<column>'] <= upper_bound)].copy()\n",
    "\n",
    "# Opción 2: Reemplazar outliers con los límites (winsorización)\n",
    "df['<column>_winsorized'] = df['<column>'].copy()\n",
    "df.loc[df['<column>'] < lower_bound, '<column>_winsorized'] = lower_bound\n",
    "df.loc[df['<column>'] > upper_bound, '<column>_winsorized'] = upper_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Selección de características**\n",
    "\n",
    "✔️ Cuando tenemos un número grande de características y queremos simplificar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminar características constantes o cuasi constantes\n",
    "✔️ Eliminar siempre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold(threshold=0) # seleccionar características con varianza mayor a 0 o el valor que se desee\n",
    "sel.fit(X_train)\n",
    "\n",
    "X_train = sel.transform(X_train)\n",
    "X_test = sel.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selección univariada\n",
    "\n",
    "Funciones de evaluación:\n",
    "- Para tareas de regresión: f_regression, mutual_info_regression\n",
    "\n",
    "- Para tareas de clasificación: chi2, f_classif, mutual_info_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile, chi2\n",
    "\n",
    "# K-Best\n",
    "X_new = SelectKBest(chi2, k=2).fit_transform(X, y)\n",
    "\n",
    "# Percentile\n",
    "X_new = SelectPercentile(chi2, percentile=10).fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matriz de correlación\n",
    "\n",
    "- Las buenas variables están altamente correlacionadas con el objetivo.\n",
    "- Las variables deben estar correlacionadas con el objetivo pero no correlacionadas entre sí."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df.corr()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool_))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
    "df_new = df.drop(df.columns[to_drop], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métodos de envoltura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "sfs = SFS(RandomForestRegressor(), \n",
    "           k_features=10, \n",
    "           forward=False, # True para forward selection, False para backward selection\n",
    "           floating=False, \n",
    "           verbose=2,\n",
    "           scoring='r2',\n",
    "           cv=3)\n",
    "\n",
    "sfs = sfs.fit(np.array(X_train), y_train)\n",
    "\n",
    "X_train.columns[list(sfs1.k_feature_idx_)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métodos integrados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "## Selección de características con Lasso\n",
    "sel_ = SelectFromModel(Lasso(alpha=100))\n",
    "sel_.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "selected_feat = X_train.columns[(sel_.get_support())]\n",
    "print('Características totales: {}'.format((X_train.shape[1])))\n",
    "print('Características seleccionadas: {}'.format(len(selected_feat)))\n",
    "print('Características con coeficientes que se reducen a cero: {}'.format( np.sum(sel_.estimator_.coef_ == 0)))\n",
    "\n",
    "\n",
    "## Para ver la importancia de las características\n",
    "lasso = Lasso(alpha=0.1)  # Ajustar alpha según sea necesario\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "importance = pd.DataFrame({'Feature': X_train.columns, 'Importance': np.abs(lasso.coef_)})\n",
    "importance = importance.sort_values('Importance', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criterios de métodos para selección:\n",
    "\n",
    "### Entrada numérica, salida numérica\n",
    "\n",
    "- Este es un problema de modelado predictivo de regresión con variables de entrada numérica.\n",
    "\n",
    "- Las técnicas más comunes son utilizar un **coeficiente de correlación**, como la de Pearson para una correlación lineal o métodos basados ​​en rango para una correlación no lineal.\n",
    "\n",
    "- Las pruebas empleadas son:\n",
    "    - Coeficiente de correlación de Pearson (lineal).\n",
    "    - Coeficiente de rango de Spearman (no lineal)\n",
    "\n",
    "\n",
    "### Entrada numérica, salida categórica\n",
    "\n",
    "- Este es un problema de modelado predictivo de clasificación con variables de entrada numérica.\n",
    "\n",
    "- Este podría ser el ejemplo más común de un problema de clasificación,\n",
    "\n",
    "- Una vez más, las técnicas más comunes se basan en la **correlación**, aunque en este caso, deben tener en cuenta el **objetivo categórico**.\n",
    "\n",
    "- Podemos emplear las siguientes pruebas:\n",
    "\n",
    "    - Coeficiente de correlación ANOVA (lineal).\n",
    "    - Coeficiente de rango de Kendall (no lineal).\n",
    "\n",
    "- Kendall supone que la variable categórica es ordinal.\n",
    "\n",
    "\n",
    "### Entrada categórica, salida numérica\n",
    "\n",
    "- Este es un problema de modelado predictivo de regresión con variables de entrada categóricas.\n",
    "\n",
    "- Este es un ejemplo extraño de un problema de regresión (no lo encontraremos a menudo).\n",
    "\n",
    "- Podemos usar los **mismos métodos de \"entrada numérica, salida categórica\"** ​​(descritos anteriormente), pero **al revés**.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Entrada categórica, salida categórica\n",
    "\n",
    "- Este es un problema de modelado predictivo de clasificación con variables de entrada categóricas.\n",
    "\n",
    "- La medida de correlación más común para los datos categóricos es la prueba de **chi cuadrado**.También podemos usar información mutua (ganancia de información) del campo de la teoría de la información.\n",
    "\n",
    "- Las siguientes pruebas se pueden emplear en este caso:\n",
    "\n",
    "    - Prueba de chi cuadrado (tablas de contingencia).\n",
    "    - Información mutua.\n",
    "\n",
    "        - De hecho, la información mutua es un método poderoso que puede resultar útil para datos categóricos y numéricos, p.Es agnóstico para los tipos de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Creación de características**\n",
    "\n",
    "✔️ Cuando tenermos pocas características o necesitamos mejorar el performance del modelo a través de nuevas variables.\n",
    " \n",
    "1. Transformaciones matemáticas\n",
    "2. Conteo\n",
    "3. Fraccionar valores en sus partes (Ids, teléfonos, direcciones, etc)\n",
    "4. Transformación de grupo\n",
    "\n",
    "### Pautas:\n",
    "- Los **modelos lineales** aprenden **sumas y restas** naturalmente, pero no pueden aprender nada más complejo.\n",
    "- Las **proporciones parecen ser difíciles de aprender** para la mayoría de los modelos. \n",
    "    - Las **combinaciones de relación** a menudo conducen a algunas ganancias de rendimiento fáciles.\n",
    "- Los **modelos lineales y las redes neuronales** generalmente funcionan mejor con **características normalizadas**. \n",
    "    - Las **redes neuronales** especialmente necesitan **características escaladas** a valores no muy lejos de 0. \n",
    "    - Los modelos basados ​​en **árboles** (como bosques aleatorios y xgboost) a veces pueden beneficiarse de la **normalización**, pero en general no.\n",
    "- Los modelos de **árboles** pueden aprender a aproximar casi cualquier combinación de características, pero cuando una **combinación** es especialmente importante, aún se puede obtener beneficios al crearla explícitamente, especialmente cuando los datos son limitados.\n",
    "- Los **recuentos son especialmente útiles para los modelos de árboles**, ya que estos modelos no tienen una forma natural de agregar información en muchas características a la vez.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
